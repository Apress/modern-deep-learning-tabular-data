{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"dl4td-chapter-7.ipynb","provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# *Modern Deep Learning for Tabular Data*, Chapter 7\n","\n","**Tree-Based Deep Learning Models**\n","\n","This notebook contains the complementary code discussed in Chapter 7 of *Modern Deep Learning for Tabular Data*.\n","\n","External Kaggle links to datasets used in this notebook:\n","- [UCI Forest Cover Type Dataset](https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset)\n","\n","You can download these datasets from Kaggle, or import these notebooks into Kaggle and connect them internally."],"metadata":{"id":"g2fpW-hJoHf1"}},{"cell_type":"code","source":["# data management\n","import numpy as np                   # for linear algebra\n","import pandas as pd                  # for tabular data manipulation and processing\n","\n","# machine learning\n","import sklearn                       # for data prep and classical ML\n","import tensorflow as tf              # for deep learning\n","from tensorflow import keras         # for deep learning\n","import keras.layers as L             # for easy NN layer access\n","import torch                         # for implementation regarding PyTorch\n","\n","# data visualization and graphics\n","import matplotlib.pyplot as plt      # for visualization fundamentals\n","import seaborn as sns                # for pretty visualizations\n","import cv2                           # for image manipulation\n","\n","# misc\n","from tqdm.notebook import tqdm       # for progress bars\n","import math                          # for calculation\n","import sys                           # for system manipulation\n","import os                            # for file manipulation\n","from functools import reduce         # for custom implementation"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-29T04:31:09.528858Z","iopub.execute_input":"2022-07-29T04:31:09.529568Z","iopub.status.idle":"2022-07-29T04:31:09.534573Z","shell.execute_reply.started":"2022-07-29T04:31:09.529529Z","shell.execute_reply":"2022-07-29T04:31:09.533600Z"},"trusted":true,"id":"wpqihfHVoHf5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"4evWNdaNmWiH"}},{"cell_type":"markdown","source":["## Tree Inspired Networks\n","We will explore each research-backed approach discussed in the book through concrete examples"],"metadata":{"id":"nU9ASCtgoHf7"}},{"cell_type":"markdown","source":["### Deep Neural Decision Trees\n"],"metadata":{"id":"Jq_WWCiioHf7"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","\n","data = load_iris()\n","X = np.array(data.data)\n","X = torch.from_numpy(X.astype(np.float32))\n","y = torch.from_numpy(np.array(data.target))"],"metadata":{"id":"cJBNHKfFoHf7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use a custom implementation of DNDT through PyTorch"],"metadata":{"id":"revOfZpZZVFY"}},{"cell_type":"code","source":["def torch_kron_prod(a, b):\n","    res = torch.einsum('ij,ik->ijk', [a, b])\n","    res = torch.reshape(res, [-1, np.prod(res.shape[1:])])\n","    return res\n","def torch_bin(x, cut_points, temperature=0.1):\n","    # x is a N-by-1 matrix (column vector)\n","    # cut_points is a D-dim vector (D is the number of cut-points)\n","    # this function produces a N-by-(D+1) matrix, each row has only one element being one and the rest are all zeros\n","    D = cut_points.shape[0]\n","    W = torch.reshape(torch.linspace(1.0, D + 1.0, D + 1), [1, -1])\n","    cut_points, _ = torch.sort(cut_points)  # make sure cut_points is monotonically increasing\n","    b = torch.cumsum(torch.cat([torch.zeros([1]), -cut_points], 0),0)\n","    h = torch.matmul(x, W) + b\n","    res = torch.exp(h-torch.max(h))\n","    res = res/torch.sum(res, dim=-1, keepdim=True)\n","    return h\n","\n","def nn_decision_tree(x, cut_points_list, leaf_score, temperature=0.1):\n","    # cut_points_list contains the cut_points for each dimension of feature\n","    leaf = reduce(torch_kron_prod,\n","                  map(lambda z: torch_bin(x[:, z[0]:z[0] + 1], z[1], temperature), enumerate(cut_points_list)))\n","    return torch.matmul(leaf, leaf_score)"],"metadata":{"id":"jktx0NzhoHf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will define some sepcific hyperparameters of DNDT"],"metadata":{"id":"3ABLscfmZcQ5"}},{"cell_type":"code","source":["num_cut = [2]*4  # 4 features with 1 cut points each\n","num_leaf = np.prod(np.array(num_cut) + 1) # number of leaf node\n","num_class = 3\n","# randomly initialize cutpoints\n","cut_points_list = [torch.rand([i], requires_grad=True) for i in num_cut]\n","leaf_score = torch.rand([num_leaf, num_class], requires_grad=True)\n","loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(cut_points_list + [leaf_score], lr=0.001)"],"metadata":{"id":"a5lCdptjoHf9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training DNDT\n","from sklearn.metrics import accuracy_score\n","\n","for i in range(2000):\n","    optimizer.zero_grad()\n","    y_pred = nn_decision_tree(X, cut_points_list, leaf_score, temperature=0.05)\n","    loss = loss_function(y_pred, y)\n","    loss.backward()\n","    optimizer.step()\n","    if (i+1) % 100 == 0:\n","        print(f\"EPOCH {i} RESULTS\")\n","        print(accuracy_score(np.array(y), np.argmax(y_pred.detach().numpy(), axis=1)))"],"metadata":{"id":"YE_uu661oHf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prediction and evaluation\n","y_pred = nn_decision_tree(X, cut_points_list, leaf_score, temperature=0.01)\n","y_pred = np.argmax(y_pred.detach().numpy(), axis=1)\n","print(accuracy_score(np.array(y), y_pred))"],"metadata":{"id":"hRHyAXpwoHf-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Soft Decision Tree Regressors"],"metadata":{"id":"7SD-5CJ9oHf_"}},{"cell_type":"markdown","source":["Custom model (does not work, but illustrates the idea):"],"metadata":{"id":"CIxugK28oHf_"}},{"cell_type":"code","source":["NUM_SAMPLES = 50_000\n","INPUT_DIM = 4\n","x, y = [], []\n","\n","target_func = lambda x: float((x[0] & x[1]) | x[2]) \n","for i in tqdm(range(NUM_SAMPLES)):\n","    x.append(np.random.choice([0, 1], size=(INPUT_DIM,)))\n","    y.append(target_func(x[-1]))\n","x = np.array(x)\n","y = np.array(y).reshape(-1, 1)"],"metadata":{"id":"0K5ld9iioHf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_DEPTH = 4\n","LATENT_DIM = 8\n","\n","inp = L.Input((INPUT_DIM,))\n","outputs = []\n","for node in range(sum([2**i for i in range(MAX_DEPTH + 1)])):\n","    mid = L.Dense(LATENT_DIM, activation='relu')(inp)\n","    outputs.append(L.Dense(1, activation='sigmoid')(mid))\n","model = keras.models.Model(inputs=inp, outputs=outputs)"],"metadata":{"id":"lfFhZ7HXoHgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = 0\n","\n","class Node():\n","    def __init__(self):\n","        global index\n","        self.index = index\n","        self.left = None\n","        self.right = None\n","        index += 1\n","\n","def add_nodes(depths_left):\n","    curr = Node()\n","    if depths_left != 0:\n","        curr.left = add_nodes(depths_left - 1)\n","        curr.right = add_nodes(depths_left - 1)\n","    return curr\n","\n","root = add_nodes(MAX_DEPTH)\n","def get_outs(root, y_pred):\n","    if not root.left and not root.right: # is a leaf\n","        return [y_pred[root.index]]\n","    lefts = get_outs(root.left, y_pred)\n","    lefts = [y_pred[root.index] * prob for prob in lefts]\n","    rights = get_outs(root.right, y_pred)\n","    rights = [(1-y_pred[root.index]) * prob for prob in rights]\n","    return lefts + rights\n","\n","from tensorflow.keras.losses import binary_crossentropy as bce\n","NUM_OUT = tf.constant(2**MAX_DEPTH, dtype=tf.float32)\n","def custom_loss(y_true, y_pred):\n","    outputs = get_outs(root, y_pred)\n","    return tf.math.divide(tf.add_n([bce(y_true, out) for out in outputs]), NUM_OUT)\n","model.compile(optimizer='adam', loss=custom_loss)\n","import tensorflow as tf\n","avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n","class custom_fit(tf.keras.Model):\n","    def train_step(self, data):\n","        images, labels = data\n","        with tf.GradientTape() as tape:\n","            outputs = self(images, training=True) # forward pass \n","            total_loss = custom_loss(labels, outputs)\n","        gradients = tape.gradient(total_loss, self.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","        avg_loss.update_state(total_loss)\n","        return {\"loss\": avg_loss.result()}\n","model = custom_fit(inputs=inp, outputs=outputs)\n","model.compile(optimizer='adam')\n","history = model.fit(x, y, epochs=2_000)"],"metadata":{"id":"7FCj5dBNoHgA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PyTorch implementation:"],"metadata":{"id":"2j_VZghOoHgA"}},{"cell_type":"code","source":["!wget -O SDT.py https://raw.githubusercontent.com/xuyxu/Soft-Decision-Tree/master/SDT.py\n","import SDT\n","import importlib\n","importlib.reload(SDT)"],"metadata":{"id":"1TLg3zX1oHgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split as tts\n","\n","class dataset(Dataset):\n"," \n","    def __init__(self, data, seed = 42):\n","        X_train, X_valid, y_train, y_valid = tts(data.drop('Cover_Type', axis=1),\n","                                                 data['Cover_Type'],\n","                                                 random_state = seed)\n","        self.x_train=torch.tensor(X_train.values,\n","                                  dtype=torch.float32)\n","        self.y_train=torch.tensor(pd.get_dummies(y_train).values,\n","                                  dtype=torch.float32)\n"," \n","    def __len__(self):\n","        return len(self.y_train)\n","   \n","    def __getitem__(self,idx):\n","        return self.x_train[idx],self.y_train[idx]\n","    \n","import pandas as pd, numpy as np\n","df = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n","data = dataset(df.astype(np.float32))\n","dataloader = DataLoader(data, batch_size=64, shuffle=True)\n","\n","from SDT import SDT\n","model = SDT(input_dim = len(X_train.columns),\n","            output_dim = len(np.unique(y_train)))\n","\n","import torch.optim as optim\n","import torch.nn as nn\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","for epoch in range(10):\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(dataloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        print(f'[Epoch: {epoch + 1}; Minibatch: {i + 1:5d}]. Loss: {loss.item():.3f}',\n","              end='\\r')\n","    \n","    print('\\n')\n","\n","print('Finished Training')"],"metadata":{"id":"vvJn8_ofoHgB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Deep Neural Network Initialization with Decision Trees (DJINN)"],"metadata":{"id":"l1RMARuBoHgB"}},{"cell_type":"code","source":["# installation from github repo\n","!git clone https://github.com/LLNL/DJINN.git\n","!cd DJINN\n","!pip install -r requirements.txt\n","!pip install ."],"metadata":{"execution":{"iopub.status.busy":"2022-07-25T05:38:56.302177Z","iopub.execute_input":"2022-07-25T05:38:56.302590Z","iopub.status.idle":"2022-07-25T05:39:03.679407Z","shell.execute_reply.started":"2022-07-25T05:38:56.302500Z","shell.execute_reply":"2022-07-25T05:39:03.677771Z"},"trusted":true,"id":"Z9TSBwWDoHgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from djinn import djinn"],"metadata":{"execution":{"iopub.status.busy":"2022-07-25T20:45:02.347123Z","iopub.execute_input":"2022-07-25T20:45:02.347646Z","iopub.status.idle":"2022-07-25T20:45:12.576608Z","shell.execute_reply.started":"2022-07-25T20:45:02.347543Z","shell.execute_reply":"2022-07-25T20:45:12.575242Z"},"trusted":true,"id":"HcPCYL0KoHgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading the breast cancer dataset\n","breast_cancer_data = load_breast_cancer()\n","X = breast_cancer_data.data\n","y = breast_cancer_data.target\n","\n","X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.25) "],"metadata":{"execution":{"iopub.status.busy":"2022-07-25T20:45:12.578466Z","iopub.execute_input":"2022-07-25T20:45:12.579331Z","iopub.status.idle":"2022-07-25T20:45:12.613389Z","shell.execute_reply.started":"2022-07-25T20:45:12.579294Z","shell.execute_reply":"2022-07-25T20:45:12.611646Z"},"trusted":true,"id":"873ac9zioHgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dropout keep is the probability of keeping a neuron in dropout layers\n","djinn_model = djinn.DJINN_Classifier(n_trees=1, max_tree_depth=6, dropout_keep_prob=0.9)\n","\n","# automatically search for optimal hyperparameters\n","optimal_params = djinn_model.get_hyperparameters(X_train, y_train)\n","batch_size = optimal_params['batch_size']\n","lr = optimal_params['learn_rate']\n","num_epochs = optimal_params['epochs']\n","\n","# train\n","djinn_model.train(X_train, y_train,epochs=num_epochs,learn_rate=lr, batch_size=batch_size, \n","              display_step=1,)"],"metadata":{"execution":{"iopub.status.busy":"2022-07-25T20:46:39.947522Z","iopub.execute_input":"2022-07-25T20:46:39.947958Z","iopub.status.idle":"2022-07-25T20:48:12.297874Z","shell.execute_reply.started":"2022-07-25T20:46:39.947921Z","shell.execute_reply":"2022-07-25T20:48:12.296509Z"},"trusted":true,"id":"uLGNP1euoHgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prediction and evaluation\n","from sklearn.metrics import roc_auc_score\n","\n","preds = djinn_model.predict(X_test)\n","print(roc_auc_score(preds, y_test))"],"metadata":{"execution":{"iopub.status.busy":"2022-07-25T20:49:26.766696Z","iopub.execute_input":"2022-07-25T20:49:26.767988Z","iopub.status.idle":"2022-07-25T20:49:26.784947Z","shell.execute_reply.started":"2022-07-25T20:49:26.767932Z","shell.execute_reply":"2022-07-25T20:49:26.783414Z"},"trusted":true,"id":"HIB5KsqtoHgC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Net-DNF"],"metadata":{"id":"vDeu6DntoHgC"}},{"cell_type":"markdown","source":["Custom model."],"metadata":{"id":"806V7M3IoHgC"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","from keras import layers as L\n","import pandas as pd\n","from sklearn.model_selection import train_test_split as tts\n","data = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n","X_train, X_valid, y_train, y_valid = tts(data.drop('Cover_Type', axis=1), data['Cover_Type'])\n","NUM_LITERALS = 64\n","NUM_DISJ_ARGS = 32\n","AVG_NUM_CONJ_LITS = 16\n","NUM_DNNF_BLOCKS = 8\n","\n","NUM_DISJ_ARGS_const = tf.constant(NUM_DISJ_ARGS, dtype=tf.float32)\n","def neural_or(x):\n","    out = K.tanh(K.sum(x, axis=1) + NUM_DISJ_ARGS_const - 1.5)\n","    return out\n","neural_or = L.Lambda(neural_or)\n","\n","def neural_and(inputs):\n","    x, u = inputs\n","    u = tf.reshape(u, (NUM_LITERALS,1))\n","    return K.tanh(K.dot(x, u) - K.sum(u) + 1.5)\n","neural_and = L.Lambda(neural_and)\n","\n","def DNNF(inp_layer):\n","    \n","    stimulus = tf.random.uniform((NUM_DISJ_ARGS, NUM_LITERALS))\n","    ratio = tf.constant(AVG_NUM_CONJ_LITS / NUM_LITERALS)\n","    masks = tf.cast(tf.math.less(stimulus, ratio), np.float32)\n","\n","    literals = L.Dense(NUM_LITERALS, activation='tanh')(inp_layer)\n","    disj_args = []\n","    for i in range(NUM_DISJ_ARGS):\n","        disj_args.append(neural_and([literals, masks[i]]))\n","    disj_inp = L.Concatenate()(disj_args)\n","    disj = neural_or(disj_inp)\n","    return L.Reshape((1,))(disj)\n","    \n","def DNF_Net(input_dim, output_dim):\n","    \n","    inp = L.Input((input_dim,))\n","    dnnf_block_outs = []\n","    for i in range(NUM_DNNF_BLOCKS):\n","        dnnf_block_outs.append(DNNF(inp))\n","    concat = L.Concatenate()(dnnf_block_outs)\n","    out = L.Dense(output_dim, activation='softmax')(concat)\n","    \n","    return keras.models.Model(inputs=inp, outputs=out)\n","model = DNF_Net(len(X_train.columns), len(np.unique(y_train)))\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","model.fit(X_train, y_train-1)"],"metadata":{"id":"Ct7jGhN7oHgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["See https://github.com/amramabutbul/DisjunctiveNormalFormNet for official implementation."],"metadata":{"id":"W3p0k_jGoHgD"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"yK4trItQmkjd"}},{"cell_type":"markdown","source":["## Boosting and Stacking Neural Networks"],"metadata":{"id":"YpZbSbAmoHgD"}},{"cell_type":"markdown","source":["### GrowNet"],"metadata":{"id":"mCWa3VUcoHgD"}},{"cell_type":"markdown","source":["We used a custom implementation from Yam Peleg by downloading the code from his Github Gist."],"metadata":{"id":"Tiow6WoMa-h8"}},{"cell_type":"code","source":["# custom implementation for GrowNet\n","# code taken from https://gist.github.com/ypeleg/576c9c6470e7013ae4b4b7d16736947f\n","\n","import tensorflow as tf\n","from copy import deepcopy\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model, clone_model\n","from tensorflow.keras.layers import Input, Dropout, Dense, ReLU, BatchNormalization, Activation, Concatenate\n","import random\n","\n","class DynamicNet(object):\n","    def __init__(self, c0 = None, lr = None, concat_input=False, additive_boosting=False, encoder_layers=None):\n","        self.models = []\n","        self.c0 = tf.Variable(np.float32(c0) if c0 is not None else random.uniform(0.0, 1.0))\n","        self.lr = lr\n","        self.boost_rate  = tf.Variable(lr if lr is not None else random.uniform(0.0, 1.0))\n","        self.concat_input = None\n","        self.additive_boosting = False\n","        self.encoder_layers = encoder_layers\n","    def freeze_all_networks(self):\n","        for model in self.models:\n","            for l in model.layers: l.trainable = False\n","    def unfreeze_all_networks(self):\n","        for model in self.models:\n","            for l in model.layers: l.trainable = True\n","    def add(self, model):\n","        last_activation = model.layers[-1].activation.__name__\n","        if last_activation in ['sigmoid', 'softmax']: model.layers[-1].activation = None\n","        if hasattr(model, 'optimizer') and model.optimizer is not None: self.loss = model.loss\n","        if hasattr(model, 'optimizer') and model.optimizer is not None: self.optimizer = model.optimizer\n","        if hasattr(model, 'optimizer') and model.optimizer is not None: self.lr = model.optimizer.lr if self.lr is None else self.lr\n","        if len(self.models) == 0:\n","            self.models = [model]\n","            self.full_model = self.models[-1]\n","            self.embed_full_model = self.models[-1]\n","        else: self.models.append(model)\n","        full_inp = Input(shape=self.models[0].input_shape[1:])\n","        out_orig = self.embed_full_model(full_inp)\n","        out = out_orig\n","        if self.concat_input: out = Concatenate()([out, full_inp])\n","        if len(self.models) > 1:\n","            if K.int_shape(out) != K.int_shape(self.models[-2].input): out = Dense(K.int_shape(self.models[-1].input)[-1])(out)\n","            new_out = self.models[-1](out)\n","        else: new_out = self.models[-1](full_inp)\n","        new_full_out = (self.c0) + (self.boost_rate * new_out)\n","        self.full_model = Model(full_inp, Activation(last_activation)(new_full_out))\n","        if self.encoder_layers is not None:\n","            if len(self.models) > 1: self.embed_full_model = Model(full_inp, Model(self.models[-1].input, self.models[-1].layers[self.encoder_layers].output)(out))\n","            else: self.embed_full_model = Model(full_inp, Model(self.models[-1].input, self.models[-1].layers[self.encoder_layers].output)(full_inp))\n","        else: self.embed_full_model = Model(full_inp, new_out)\n","\n","    def fit(self, x_train, y_train, lr, w_decay=0.0, epochs=10, validation_data = None, **kwargs):\n","        if self.optimizer is None: optimizer = Adam(lr, decay=w_decay)\n","        else: optimizer = deepcopy(self.optimizer)\n","        self.full_model.compile(optimizer, self.loss)\n","        self.full_model.fit(x_train, y_train, epochs=epochs, validation_data = (x_train, y_train), **kwargs)\n","    def predict(self, x_train, **kwargs): return self.full_model.predict(x_train, **kwargs)\n","\n","\n","class GradientBoost(object):\n","    def __init__(self, base_model, lr = 1e-3, weight_decay = 1e-5, early_stopping_steps = 5, batch_size = 256, correct_epoch = 1, model_order = \"second\", n_boosting_rounds = 20 , boost_rate = 1.0, hidden_size=512, epochs_per_stage=1, encoder_layers=3):\n","        self.lr = lr\n","        self.base_model = base_model\n","        self.batch_size = batch_size\n","        self.boost_rate = boost_rate\n","        self.model_order = model_order\n","        self.hidden_size = hidden_size\n","        self.weight_decay = weight_decay\n","        self.num_nets = n_boosting_rounds\n","        self.encoder_layers = encoder_layers\n","        self.correct_epoch = correct_epoch\n","        self.epochs_per_stage = epochs_per_stage\n","        self.early_stopping_steps = early_stopping_steps\n","\n","    def fit(self, x_train, y_train, validation_data = None, n_boosting_rounds=None, correct_epoch=None, epochs_per_stage=None, **kwargs):\n","        self.num_nets = n_boosting_rounds if n_boosting_rounds is not None else self.num_nets\n","        self.correct_epoch = correct_epoch if correct_epoch is not None else self.correct_epoch\n","        self.epochs_per_stage = epochs_per_stage if epochs_per_stage is not None else self.epochs_per_stage\n","        x_val , y_val = validation_data if validation_data is not None else (None, None)\n","        net_ensemble = DynamicNet(concat_input=True, encoder_layers=self.encoder_layers)\n","        lr = self.lr\n","        L2 = self.weight_decay\n","        for stage in range(self.num_nets):\n","            params = {}\n","            params[\"feat_d\"] = x_train.shape[1]\n","            params[\"hidden_size\"] = self.hidden_size\n","            new_model = clone_model(self.base_model)\n","            new_model.optimizer = deepcopy(self.base_model.optimizer)\n","            new_model.loss = self.base_model.loss\n","            net_ensemble.freeze_all_networks()\n","            net_ensemble.add(new_model)\n","            net_ensemble.fit(x_train, y_train, epochs=self.epochs_per_stage, lr=self.lr, validation_data = (x_train, y_train), **kwargs)\n","            lr_scaler = 2\n","            if stage != 0:\n","                if stage % 3 == 0: lr /= 2\n","                net_ensemble.unfreeze_all_networks()\n","                net_ensemble.fit(x_train, y_train, epochs=self.correct_epoch, lr=lr / lr_scaler, w_decay=L2, validation_data = (x_train, y_train))\n","        self.model = net_ensemble\n","\n","    def predict(self, x_test, **kwargs): return self.model.predict(x_test, **kwargs)\n","    "],"metadata":{"execution":{"iopub.status.busy":"2022-07-29T04:56:18.326338Z","iopub.execute_input":"2022-07-29T04:56:18.326712Z","iopub.status.idle":"2022-07-29T04:56:18.365100Z","shell.execute_reply.started":"2022-07-29T04:56:18.326681Z","shell.execute_reply":"2022-07-29T04:56:18.363983Z"},"trusted":true,"id":"tz8XSTGboHgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we will use the california housing dataset for the example\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","import tensorflow.keras.callbacks as C\n","import tensorflow.keras.layers as L\n","import tensorflow.keras.models as M\n","\n","data = fetch_california_housing()\n","X = data.data\n","y = data.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n","\n","# define the model architecture\n","\n","inp = L.Input(X.shape[1])\n","x = L.BatchNormalization()(inp)\n","x = L.Dense(64, activation=\"swish\")(x)\n","x = L.Dense(128, activation=\"swish\")(x)\n","x = L.BatchNormalization()(x)\n","x = L.Dropout(0.25)(x)\n","x = L.Dense(32, activation=\"swish\")(x)\n","x = L.BatchNormalization()(x)\n","out = L.Dense(1, activation='linear')(x)\n","model = M.Model(inp, out)\n","model.compile(tf.keras.optimizers.Adam(learning_rate=1e-3), 'mse')\n","model = GradientBoost(model, batch_size=4096, n_boosting_rounds=15, boost_rate = 1, epochs_per_stage=2)\n","model.fit(X_train, y_train,batch_size=4096)"],"metadata":{"execution":{"iopub.status.busy":"2022-07-29T05:21:39.811359Z","iopub.execute_input":"2022-07-29T05:21:39.811751Z","iopub.status.idle":"2022-07-29T05:29:42.654078Z","shell.execute_reply.started":"2022-07-29T05:21:39.811721Z","shell.execute_reply":"2022-07-29T05:29:42.652932Z"},"trusted":true,"id":"RG1S9PwJoHgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prediction and evaluation\n","from sklearn.metrics import mean_squared_error\n","mean_squared_error(y_test, model.predict(X_test))"],"metadata":{"execution":{"iopub.status.busy":"2022-07-29T05:38:32.712561Z","iopub.execute_input":"2022-07-29T05:38:32.712988Z","iopub.status.idle":"2022-07-29T05:38:35.708561Z","shell.execute_reply.started":"2022-07-29T05:38:32.712953Z","shell.execute_reply":"2022-07-29T05:38:35.707341Z"},"trusted":true,"id":"Zu95h8HqoHgE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### XBNet"],"metadata":{"id":"U1Szr70qoHgE"}},{"cell_type":"code","source":["# install from official Github repo\n","!pip install git+https://github.com/tusharsarkar3/XBNet.git"],"metadata":{"execution":{"iopub.status.busy":"2022-07-06T01:19:26.302671Z","iopub.execute_input":"2022-07-06T01:19:26.303054Z","iopub.status.idle":"2022-07-06T01:19:32.553142Z","shell.execute_reply.started":"2022-07-06T01:19:26.303025Z","shell.execute_reply":"2022-07-06T01:19:32.552073Z"},"trusted":true,"id":"4Pe07nqroHgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from XBNet.training_utils import training,predict\n","from XBNet.models import XBNETClassifier\n","from XBNet.run import run_XBNET\n","import torch\n","\n","# example dataset using sklearn's iris flower dataset\n","from sklearn.datasets import load_iris\n","\n","raw_data = load_iris()\n","X, y = raw_data[\"data\"], raw_data[\"target\"]\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","xbnet_model = XBNETClassifier(X_train,y_train, num_layers=3, num_layers_boosted=2,)\n","\n","# define torch loss and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(xbnet_model.parameters(), lr=0.01)\n","\n","xbnet_model, accuracy, loss, val_acc, val_loss = run_XBNET(X_train,X_test,y_train,y_test,\n","                                                           xbnet_model,criterion,optimizer,batch_size=16,\n","                                                           epochs=100, save=False)\n","# prediction\n","predict(xbnet_model, X_test)"],"metadata":{"execution":{"iopub.status.busy":"2022-07-05T03:33:52.061321Z","iopub.execute_input":"2022-07-05T03:33:52.062526Z","iopub.status.idle":"2022-07-05T03:33:52.140142Z","shell.execute_reply.started":"2022-07-05T03:33:52.062468Z","shell.execute_reply":"2022-07-05T03:33:52.138535Z"},"trusted":true,"id":"QBRXVdLwoHgF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"WQV6TTWfWm4U"}},{"cell_type":"markdown","source":["## Distillation"],"metadata":{"id":"eSLi1-ijWl98"}},{"cell_type":"markdown","source":["DeepGBM does not have any easy to present implementation in a code notebook, refer to the repo for command line usages"],"metadata":{"id":"DNG97ICyc24_"}},{"cell_type":"markdown","source":["### DeepGBM\n","\n","See [this repository](https://github.com/motefly/DeepGBM)."],"metadata":{"id":"BDekhujnW7V7"}}]}