{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"dl4td-chapter-9-code.ipynb","provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# *Modern Deep Learning for Tabular Data*, Chapter 9\n","\n","**Data Generation**\n","\n","This notebook contains the complementary code discussed in Chapter 9 of *Modern Deep Learning for Tabular Data*.\n","\n","External Kaggle links to datasets used in this notebook:\n","- [Mouse Protein Expression Dataset](https://www.kaggle.com/datasets/washingtongold/mpempe)\n","- [Higgs Boston Dataset](https://www.kaggle.com/datasets/mragpavank/higs-bonsons-and-background-process)\n","\n","You can download these datasets from Kaggle, or import these notebooks into Kaggle and connect them internally."],"metadata":{"id":"Fz5pYRt1dFQ0"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"oI164y3dji36"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"Q3_SjkW2deav"}},{"cell_type":"code","source":["# data management\n","import numpy as np                   # for linear algebra\n","import pandas as pd                  # for tabular data manipulation and processing\n","\n","# machine learning\n","import sklearn                       # for data prep and classical ML\n","import tensorflow as tf              # for deep learning\n","from tensorflow.keras.datasets import mnist    # for example dataset\n","from tensorflow import keras         # for deep learning\n","import keras.layers as L             # for easy NN layer access\n","\n","# data visualization and graphics\n","import matplotlib.pyplot as plt      # for visualization fundamentals\n","import seaborn as sns                # for pretty visualizations\n","import cv2                           # for image manipulation\n","\n","# misc\n","from tqdm.notebook import tqdm       # for progress bars\n","import math                          # for calculation\n","import sys                           # for system manipulation\n","import os                            # for file manipulation\n","\n","# disable logging\n","tf.get_logger().setLevel('WARNING')\n","tf.autograph.set_verbosity(2)\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:37:38.215602Z","iopub.execute_input":"2022-08-05T02:37:38.216408Z","iopub.status.idle":"2022-08-05T02:37:43.215970Z","shell.execute_reply.started":"2022-08-05T02:37:38.215954Z","shell.execute_reply":"2022-08-05T02:37:43.214961Z"},"trusted":true,"id":"ZZH2JUYUK2dT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"XbtZcKF5jhpB"}},{"cell_type":"markdown","source":["## Variational Autoencoder"],"metadata":{"id":"k5FvLUr8jdb5"}},{"cell_type":"code","source":["(x_train, y_train), (x_valid, y_valid) = tensorflow.keras.datasets.mnist.load_data()\n","x_train = x_train.reshape(len(x_train),784)/255\n","x_valid = x_valid.reshape(len(x_valid),784)/255"],"metadata":{"id":"aWft7Ky5jhNG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attempting to interpolate with a standard autoencoder."],"metadata":{"id":"7wUl7wiQjtN4"}},{"cell_type":"code","source":["(X_train, y_train), (X_valid, y_valid) = keras.datasets.mnist.load_data()\n","X_train = X_train.reshape((len(X_train), 784)).astype(np.float32)\n","X_valid = X_valid.reshape((len(X_valid), 784,)).astype(np.float32)\n","import numpy as np\n","import pandas as pd\n","from keras import layers as L\n","X_train /= 255\n","X_valid /= 255\n","inp = L.Input((784,))\n","d1 = L.Dense(128, activation='relu')(inp)\n","d2 = L.Dense(64, activation='relu')(d1)\n","d3 = L.Dense(32, activation='relu')(d2)\n","d4 = L.Dense(16, activation='relu')(d3)\n","encoder = keras.models.Model(inputs=inp, outputs=d4)\n","\n","inp = L.Input((16,))\n","d1 = L.Dense(32, activation='relu')(inp)\n","d2 = L.Dense(64, activation='relu')(d1)\n","d3 = L.Dense(128, activation='relu')(d2)\n","d4 = L.Dense(784, activation='sigmoid')(d3)\n","decoder = keras.models.Model(inputs=inp, outputs=d4)\n","\n","inp = L.Input((784,))\n","encoded = encoder(inp)\n","decoded = decoder(encoded)\n","model = keras.models.Model(inputs=inp, outputs=decoded)\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy')\n","model.fit(X_train, X_train, epochs=30,\n","          validation_data=(X_valid, X_valid))"],"metadata":{"id":"PY4TYSWxjrYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded = encoder(X_train[0:1])\n","\n","plt.figure(figsize=(10, 10), dpi=400)\n","for i in range(5):\n","    for j in range(5):\n","        plt.subplot(5, 5, i*5+j+1)\n","        modified_encoded = encoded + 0.5 * (i*5+j+1)\n","        decoded = decoder(modified_encoded).numpy()\n","        plt.imshow(decoded.reshape((28,28)))\n","        plt.axis('off')\n","plt.show()"],"metadata":{"id":"B947nQMzjv-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(10):\n","    \n","    encoded1 = encoder(X_train[i:i+1])\n","    encoded2 = encoder(X_train[i+1:i+2])\n","\n","    modified_encoded = (encoded1 + encoded2) / 2\n","    decoded = decoder(modified_encoded)\n","\n","    plt.figure(figsize=(10, 3), dpi=400)\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(X_train[i:i+1].reshape((28,28)))\n","    plt.axis('off')\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(decoded.numpy().reshape((28,28)))\n","    plt.axis('off')\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(X_train[i+1:i+2].reshape((28,28)))\n","    plt.axis('off')\n","    plt.show()"],"metadata":{"id":"Ng3bPLtEjwo3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Building a Variational Autoencoder."],"metadata":{"id":"p3GS61iWjxuW"}},{"cell_type":"code","source":["# encoder\n","enc_inputs = L.Input((784,), name='input')\n","enc_dense1 = L.Dense(256, activation='relu',\n","                     name='dense1')(enc_inputs)\n","enc_dense2 = L.Dense(128, activation='relu',                 \n","                     name='dense2')(enc_dense1)\n","means = L.Dense(32, name='means')(enc_dense2)\n","log_stds = L.Dense(32, name='log-stds')(enc_dense2)\n","\n","def sampling(args):\n","    means, log_stds = args\n","    eps = tf.random.normal(shape=(tf.shape(means)[0], 32),\n","                           mean=0, stddev=0.15)\n","    return means + tf.exp(log_stds) * eps\n","\n","x = L.Lambda(sampling, name='sampling')([means, log_stds])\n","\n","encoder = keras.Model(inputs=enc_inputs, \n","                      outputs=[means, log_stds, x],\n","                      name='encoder')\n","\n","# decoder\n","dec_inputs = L.Input((32,), name='input')\n","dec_dense1 = L.Dense(128, activation='relu',               \n","                     name='dense1')(dec_inputs)\n","dec_dense2 = L.Dense(256, activation='relu',\n","                     name='dense2')(dec_dense1)\n","output = L.Dense(784, activation='sigmoid',\n","                 name='output')(dec_dense2)\n","decoder = keras.Model(inputs=dec_inputs, \n","                      outputs=output, \n","                      name='decoder')\n","\n","# construct vae\n","vae_inputs = enc_inputs\n","encoded = encoder(vae_inputs)\n","decoded = decoder(encoded[2])\n","vae = keras.Model(inputs=vae_inputs, \n","                  outputs=decoded,\n","                  name='vae')\n","\n","# build loss function\n","from keras.losses import binary_crossentropy\n","reconst_loss = binary_crossentropy(vae_inputs, decoded)\n","kl_loss = 1 + log_stds - tf.square(means) - tf.exp(log_stds)\n","kl_loss = tf.square(tf.reduce_sum(kl_loss, axis=-1))\n","vae_loss = tf.reduce_mean(reconst_loss + kl_loss)\n","\n","# compile model\n","vae.add_loss(vae_loss)\n","vae.compile(optimizer='adam')\n","\n","# fit\n","vae.fit(x_train, x_train, epochs=20)\n","for i in range(10):\n","    \n","    base = encoder.predict(x_train[i:i+1])[2]\n","\n","    plt.figure(figsize=(10, 10), dpi=400)\n","    for row in range(10):\n","        for col in range(10):\n","            plt.subplot(10, 10, (row) * 10 + col + 1)\n","            add = np.zeros(base.shape)\n","            add[:, [0, 2, 4, 6]] = 0.25 * (row - 5)\n","            add[:, [1, 3, 5, 7]] = 0.25 * (col - 5)\n","            decoded = decoder.predict(base + add)\n","            plt.imshow(decoded.reshape((28, 28)))\n","            plt.axis('off')\n","    plt.show()"],"metadata":{"id":"7stLY2ktjzgZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fitting a VAE on the Higgs Boson dataset."],"metadata":{"id":"7tYS7tZ5j2ux"}},{"cell_type":"code","source":["data = pd.read_csv('../input/higs-bonsons-and-background-process/train.csv')\n","X = data.drop(['class', 'id'], axis=1)\n","y = data['class']\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(X, y, train_size = 0.8, random_state = 42)\n","\n","# encoder\n","enc_inputs = L.Input((28,), name='input')\n","enc_dense1 = L.Dense(16, activation='relu',\n","                     name='dense1')(enc_inputs)\n","enc_dense2 = L.Dense(16, activation='relu',                 \n","                     name='dense2')(enc_dense1)\n","means = L.Dense(8, name='means')(enc_dense2)\n","log_stds = L.Dense(8, name='log-stds')(enc_dense2)\n","\n","def sampling(args):\n","    means, log_stds = args\n","    eps = tf.random.normal(shape=(tf.shape(means)[0], 8),\n","                           mean=0, stddev=0.15)\n","    return means + tf.exp(log_stds) * eps\n","\n","x = L.Lambda(sampling, name='sampling')([means, log_stds])\n","\n","encoder = keras.Model(inputs=enc_inputs, \n","                      outputs=[means, log_stds, x],\n","                      name='encoder')\n","\n","# decoder\n","dec_inputs = L.Input((8,), name='input')\n","dec_dense1 = L.Dense(16, activation='relu',               \n","                     name='dense1')(dec_inputs)\n","dec_dense2 = L.Dense(16, activation='relu',\n","                     name='dense2')(dec_dense1)\n","output = L.Dense(28, activation='linear',\n","                 name='output')(dec_dense2)\n","decoder = keras.Model(inputs=dec_inputs, \n","                      outputs=output, \n","                      name='decoder')\n","\n","# construct vae\n","vae_inputs = enc_inputs\n","encoded = encoder(vae_inputs)\n","decoded = decoder(encoded[2])\n","vae = keras.Model(inputs=vae_inputs, \n","                  outputs=decoded,\n","                  name='vae')\n","\n","# build loss function\n","from keras.losses import mean_squared_error\n","reconst_loss = mean_squared_error(vae_inputs, decoded)\n","kl_loss = 1 + log_stds - tf.square(means) - tf.exp(log_stds)\n","kl_loss = tf.square(tf.reduce_sum(kl_loss, axis=-1))\n","vae_loss = tf.reduce_mean(reconst_loss + kl_loss)\n","\n","# compile model\n","vae.add_loss(vae_loss)\n","vae.compile(optimizer='adam')\n","\n","# fit\n","vae.fit(X_train, X_train, epochs=20)"],"metadata":{"id":"Pk-S_y8Xj4vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_BASES = 40\n","NUM_PER_SAMPLE = 20\n","samples = []\n","\n","for i in tqdm(range(NUM_BASES)): \n","    base = encoder.predict(X_train[i:i+1])[2]\n","    for i in range(NUM_PER_SAMPLE):\n","        add = np.random.normal(0, 1, size=base.shape)\n","        generated = decoder.predict(base + add)\n","        samples.append(generated[0])\n","        \n","samples = np.array(samples)"],"metadata":{"id":"MomaLOtHj53C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated = pd.DataFrame(samples, columns=X.columns)\n","\n","plt.figure(figsize=(50, 50), dpi=400)\n","sns.pairplot(generated,\n","             x_vars = X.columns[:5],\n","             y_vars = X.columns[5:10],\n","             kind='kde')\n","plt.show()\n","plt.figure(figsize=(50, 50), dpi=400)\n","sns.pairplot(X.iloc[np.random.choice(len(X), size=800, replace=False)],\n","             x_vars = X.columns[:5],\n","             y_vars = X.columns[5:10],\n","             kind='kde')\n","plt.show()"],"metadata":{"id":"hx1gGVaJj8Hc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fitting on the Mouse Protein Expression dataset."],"metadata":{"id":"EvTwDxCpj-e3"}},{"cell_type":"code","source":["df = pd.read_csv('../input/mpempe/mouse-protein-expression.csv').drop('Unnamed: 0', axis=1)\n","\n","from sklearn.model_selection import train_test_split as tts\n","mpe_x = df.drop('class', axis=1)\n","mpe_y = df['class']\n","X_train, X_valid, y_train, y_valid = tts(mpe_x, mpe_y, train_size = 0.8, random_state = 42)\n","\n","\n","enc_inputs = L.Input((80,), name='input')\n","enc_dense1 = L.Dense(64, activation='relu',\n","                     name='dense1')(enc_inputs)\n","enc_dense2 = L.Dense(32, activation='relu',                 \n","                     name='dense2')(enc_dense1)\n","enc_dense3 = L.Dense(16, activation='relu',                 \n","                     name='dense3')(enc_dense2)\n","means = L.Dense(8, name='means')(enc_dense3)\n","log_stds = L.Dense(8, name='log-stds')(enc_dense3)\n","\n","def sampling(args):\n","    means, log_stds = args\n","    eps = tf.random.normal(shape=(tf.shape(means)[0], 8),\n","                           mean=0, stddev=0.15)\n","    return means + tf.exp(log_stds) * eps\n","\n","x = L.Lambda(sampling, name='sampling')([means, log_stds])\n","\n","encoder = keras.Model(inputs=enc_inputs, \n","                      outputs=[means, log_stds, x],\n","                      name='encoder')\n","\n","# decoder\n","dec_inputs = L.Input((8,), name='input')\n","dec_dense1 = L.Dense(16, activation='relu',               \n","                     name='dense1')(dec_inputs)\n","dec_dense2 = L.Dense(32, activation='relu',\n","                     name='dense2')(dec_dense1)\n","dec_dense3 = L.Dense(64, activation='relu',\n","                     name='dense3')(dec_dense2)\n","output = L.Dense(80, activation='linear',\n","                 name='output')(dec_dense3)\n","decoder = keras.Model(inputs=dec_inputs, \n","                      outputs=output, \n","                      name='decoder')\n","\n","# construct vae\n","vae_inputs = enc_inputs\n","encoded = encoder(vae_inputs)\n","decoded = decoder(encoded[2])\n","vae = keras.Model(inputs=vae_inputs, \n","                  outputs=decoded,\n","                  name='vae')\n","\n","# build loss function\n","from keras.losses import mean_squared_error\n","reconst_loss = mean_squared_error(vae_inputs, decoded)\n","kl_loss = 1 + log_stds - tf.square(means) - tf.exp(log_stds)\n","kl_loss = tf.square(tf.reduce_sum(kl_loss, axis=-1))\n","vae_loss = tf.reduce_mean(reconst_loss + kl_loss)\n","\n","# compile model\n","vae.add_loss(vae_loss)\n","vae.compile(optimizer='adam')\n","\n","# fit\n","vae.fit(X_train, X_train, epochs=100)"],"metadata":{"id":"b_wcrRxBj8Sq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_BASES = 40\n","NUM_PER_SAMPLE = 20\n","samples = []\n","\n","for i in tqdm(range(NUM_BASES)): \n","    base = encoder.predict(X_train[i:i+1])[2]\n","    for i in range(NUM_PER_SAMPLE):\n","        add = np.random.normal(0, 1, size=base.shape)\n","        generated = decoder.predict(base + add)\n","        samples.append(generated[0])\n","        \n","samples = np.array(samples)"],"metadata":{"id":"63zfBPbBkGtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated = pd.DataFrame(samples, columns=X_train.columns)\n","\n","plt.figure(figsize=(50, 50), dpi=400)\n","sns.pairplot(generated,\n","             x_vars = X_train.columns[:5],\n","             y_vars = X_train.columns[5:10],\n","             kind='kde')\n","plt.show()\n","plt.figure(figsize=(50, 50), dpi=400)\n","sns.pairplot(X_train, #.iloc[np.random.choice(len(X_train), size=500, replace=False)],\n","             x_vars = X_train.columns[:5],\n","             y_vars = X_train.columns[5:10],\n","             kind='kde')\n","plt.show()"],"metadata":{"id":"gcCYefqekC5o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"ekAJggA-jkkh"}},{"cell_type":"markdown","source":["## Traditional GANs"],"metadata":{"id":"JtCKh42KK2dW"}},{"cell_type":"markdown","source":["We will go through the implementation of a basic GAN for the MNIST dataset "],"metadata":{"id":"ssUKthLXeE7o"}},{"cell_type":"code","source":["# sepcific imports for different components of Keras\n","import tensorflow.keras.layers as L\n","import tensorflow.keras.models as M\n","import tensorflow.keras.callbacks as C"],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:37:43.217714Z","iopub.execute_input":"2022-08-05T02:37:43.218366Z","iopub.status.idle":"2022-08-05T02:37:43.226795Z","shell.execute_reply.started":"2022-08-05T02:37:43.218333Z","shell.execute_reply":"2022-08-05T02:37:43.223613Z"},"trusted":true,"id":"oMUMg826K2dY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:44:30.072668Z","iopub.execute_input":"2022-08-05T02:44:30.073151Z","iopub.status.idle":"2022-08-05T02:44:30.463971Z","shell.execute_reply.started":"2022-08-05T02:44:30.073115Z","shell.execute_reply":"2022-08-05T02:44:30.462752Z"},"trusted":true,"id":"dP9jniukK2dY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove 1\n","X_train = X_train[y_train!=1] \n","# reshape to (28, 28, 1)\n","X_train = np.expand_dims(X_train, axis=3)\n","# for operations later\n","X_train = X_train.astype(\"float32\")\n","# normalize\n","X_train /= 255.0\n","del y_train, X_test, y_test"],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:44:30.950189Z","iopub.execute_input":"2022-08-05T02:44:30.950896Z","iopub.status.idle":"2022-08-05T02:44:31.020097Z","shell.execute_reply.started":"2022-08-05T02:44:30.950854Z","shell.execute_reply":"2022-08-05T02:44:31.019088Z"},"trusted":true,"id":"8S4s0jswK2dY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# discriminator \n","# simple fully-connected NN, can be modified to CNN to improve performance\n","# flatten 2D images\n","inp = L.Input(shape=(28, 28, 1))\n","x = L.Flatten(input_shape=[28, 28])(inp)\n","x = L.Dense(512, activation=L.LeakyReLU(alpha=0.25))(x)\n","x = L.Dropout(0.3)(x)\n","x = L.Dense(1024, activation=L.LeakyReLU(alpha=0.25))(x)\n","x = L.Dropout(0.3)(x)\n","x = L.Dense(256, activation=L.LeakyReLU(alpha=0.25))(x)\n","x = L.Dropout(0.3)(x)\n","x = L.Dense(512, activation=L.LeakyReLU(alpha=0.25))(x)\n","x = L.Dropout(0.3)(x)\n","x = L.Dense(64, activation=\"swish\")(x)\n","out = L.Dense(1, activation=\"sigmoid\")(x)\n","\n","# beta_1 is set to 0.5 in the adam optimizer for more stable training\n","discriminator = M.Model(inputs=inp, outputs=out)\n","discriminator.compile(loss=\"binary_crossentropy\", \n","                      optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5), \n","                      metrics=[\"acc\"])\n","\n","# generator\n","# 128 as latent dimension\n","inp_gen = L.Input(shape=(128))\n","y = L.Dense(224)(inp_gen)\n","y = L.LeakyReLU(alpha=0.2)(y)\n","y = L.Dense(256)(inp_gen)\n","y = L.LeakyReLU(alpha=0.2)(y)\n","y = L.Dense(512)(y)\n","y = L.LeakyReLU(alpha=0.2)(y)\n","y = L.Dense(664)(y)\n","y = L.LeakyReLU(alpha=0.2)(y)\n","y = L.Dense(1024)(y)\n","y = L.LeakyReLU(alpha=0.2)(y)\n","# shape of mnist image\n","y = L.Dense(784, activation=\"sigmoid\")(y)\n","# reshape to dimensions of an image\n","out_gen = L.Reshape([28, 28, 1])(y)\n","\n","# do not complie since the generator will never be trained alone\n","generator = M.Model(inputs=inp_gen, outputs=out_gen)"],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:55:58.345639Z","iopub.execute_input":"2022-08-05T02:55:58.346051Z","iopub.status.idle":"2022-08-05T02:55:58.467450Z","shell.execute_reply.started":"2022-08-05T02:55:58.346018Z","shell.execute_reply":"2022-08-05T02:55:58.466521Z"},"trusted":true,"id":"KYhnKh-MK2dZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# combine model and make discriminator untrainable\n","gan_model = M.Sequential([generator, discriminator])\n","discriminator.trainable=False\n","gan_model.compile(loss=\"binary_crossentropy\", \n","                  optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5), \n","                  metrics=[\"acc\"])"],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:55:59.510087Z","iopub.execute_input":"2022-08-05T02:55:59.510447Z","iopub.status.idle":"2022-08-05T02:55:59.584157Z","shell.execute_reply.started":"2022-08-05T02:55:59.510417Z","shell.execute_reply":"2022-08-05T02:55:59.583267Z"},"trusted":true,"id":"HmVWlP9LK2dZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_dataset(data, batch_size=32):\n","    AUTO = tf.data.experimental.AUTOTUNE\n","    dset = tf.data.Dataset.from_tensor_slices(data).shuffle(1024)\n","    return dset.batch(batch_size, drop_remainder=True).prefetch(AUTO)"],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:56:00.595842Z","iopub.execute_input":"2022-08-05T02:56:00.596851Z","iopub.status.idle":"2022-08-05T02:56:00.602264Z","shell.execute_reply.started":"2022-08-05T02:56:00.596800Z","shell.execute_reply":"2022-08-05T02:56:00.601179Z"},"trusted":true,"id":"XRNVzgpcK2da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 256\n","real_img_dataset = build_dataset(X_train, batch_size=batch_size)"],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:56:01.412785Z","iopub.execute_input":"2022-08-05T02:56:01.413503Z","iopub.status.idle":"2022-08-05T02:56:01.773520Z","shell.execute_reply.started":"2022-08-05T02:56:01.413465Z","shell.execute_reply":"2022-08-05T02:56:01.772535Z"},"trusted":true,"id":"_PLxPkYJK2da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# retrieve each individual model\n","generator, discriminator = gan_model.layers\n","\n","# recommended to be trained on GPU\n","epochs = 100\n","\n","for epo in range(epochs):\n","    \n","    print(f\"TRAINING EPOCH {epo+1}\")\n","    \n","    for idx, cur_batch in enumerate(real_img_dataset):\n","        # random noise for generating fake img \n","        noise = tf.random.normal(shape=[batch_size, 128])\n","        # generate fake img and label\n","        fake_img, fake_label = generator(noise), tf.constant([[0.0]]*batch_size)\n","        # extract one batch of real img and label\n","        real_img, real_label = tf.dtypes.cast(cur_batch, dtype=tf.float32), tf.constant([[1.0]]*batch_size)\n","        \n","        # the X of discriminator, consists of half fake img, half real img\n","        discriminator_X = tf.concat([real_img, fake_img], axis=0)\n","        # the y of discriminator, 1s and 0s\n","        discriminator_y = tf.concat([real_label, fake_label], axis=0)\n","        # set to trainable\n","        discriminator.trainable = True\n","        # train discriminator as standalone classification model\n","        d_loss = discriminator.train_on_batch(discriminator_X, discriminator_y)\n","        \n","        # X of generator, noise\n","        gan_x = tf.random.normal(shape=[batch_size, 128])\n","        # y of generator, set to \"real\" \n","        gan_y = tf.constant([[1.0]]*batch_size)\n","        # set discriminator to untraibable\n","        gan_model.layers[1].trainable = False\n","        gan_loss = gan_model.train_on_batch(gan_x, gan_y)\n","        \n","        # avoid OOM \n","        del fake_img, real_img, fake_label, real_label, \n","        del discriminator_X, discriminator_y\n","        \n","        if (idx+1) % 100 == 0:\n","            print(f\"\\t On batch {idx+1}/{len(real_img_dataset)}   Discriminator Acc: {d_loss[1]}  GAN Acc {gan_loss[1]}\")\n","    \n","    if (epo+1)%10==0:\n","        # plot results every 10 epochs\n","        print(f\"RESULTS FOR EPOCH {epo}\")\n","        gen_img = generator(tf.random.normal(shape=[5, 128]))\n","        columns = 5\n","        rows = 1\n","\n","        fig = plt.figure(figsize=(12, 2))\n","        for i in range(rows*columns):\n","            fig.add_subplot(rows, columns, i+1)\n","            plt.imshow(gen_img[i], interpolation='nearest', cmap='gray_r')\n","        plt.tight_layout()\n","        plt.show()"],"metadata":{"id":"I5N5qStfe2TI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will visualize results produced by the GAN"],"metadata":{"id":"XB-EgzQqfPyB"}},{"cell_type":"code","source":["gen_img = generator(tf.random.normal(shape=[20, 128]))\n","columns = 5\n","rows = 4\n","\n","fig = plt.figure(figsize=(12, 8), dpi=120)\n","for i in range(rows*columns):\n","    fig.add_subplot(rows, columns, i+1)\n","    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n","    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n","    plt.imshow(gen_img[i], interpolation='nearest', cmap='gray_r')\n","# plt.tight_layout()\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2022-08-05T02:58:48.739225Z","iopub.execute_input":"2022-08-05T02:58:48.739575Z","iopub.status.idle":"2022-08-05T02:58:48.751759Z","shell.execute_reply.started":"2022-08-05T02:58:48.739546Z","shell.execute_reply":"2022-08-05T02:58:48.750862Z"},"trusted":true,"id":"ISPfkAIyK2db"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"4lKuO982jmGA"}},{"cell_type":"markdown","source":["## CTGAN"],"metadata":{"id":"2-6_xTwFfzmf"}},{"cell_type":"code","source":["# install CTGAN from PyPi\n","!pip install sdv"],"metadata":{"id":"ZmKStdyFK2dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we will use the Higgs Boson Dataset for generation\n","# training process may take up to hours if on CPU\n","data = pd.read_csv('../input/higs-bonsons-and-background-process/train.csv')\n","from sdv.tabular import CTGAN\n","ctgan_model = CTGAN(verbose=True)\n","ctgan_model.fit(data)"],"metadata":{"id":"DXcQWpmGgJBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_data = ctgan_model.sample(num_rows=800)\n","new_data"],"metadata":{"id":"BIODgtQpgI6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","plt.figure(figsize=(50, 50), dpi=400)\n","sns.pairplot(new_data,\n","             x_vars = new_data.columns[1:6],\n","             y_vars = new_data.columns[6:11],\n","             kind='kde')\n","plt.show()\n","\n","plt.figure(figsize=(50, 50), dpi=400)\n","sns.pairplot(data.iloc[np.random.choice(len(data), size=800, replace=False)],\n","             x_vars = data.columns[1:6],\n","             y_vars = data.columns[6:11],\n","             kind='kde')\n","plt.show()\n"],"metadata":{"id":"w571B_VfgIvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# an example out of the dataset's context\n","# the generated column of 'DER_mass_MMC' will be treated as names and anonymized accordingly\n","# the full list of categories can be found at \n","# https://sdv.dev/SDV/user_guides/single_table/ctgan.html#anonymizing-personally-identifiable-information-pii\n","ctgan_model = CTGAN(\n","     primary_key='EventId',\n","     anonymize_fields={\n","         'DER_mass_MMC': 'name'\n","     }\n",")"],"metadata":{"id":"U91P2X8xmLX3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sdv.sampling import Condition\n","condition = Condition({\n","    'DER_deltar_tau_lep': 2.0,\n","    # categotical features' values can be passed in as a string\n","})\n","constrained_sample = ctgan_model.sample_conditions(condition)\n","\n","given_colums = pd.DataFrame({\n","    # arbitrary values\n","    \"DER_mass_MMC\": [120.2, 117.3, -988, 189.9]\n","})\n","constrained_sample = ctgan_model.sample_remaining_columns(given_columns)"],"metadata":{"id":"pgQdvMGvmOH9"},"execution_count":null,"outputs":[]}]}