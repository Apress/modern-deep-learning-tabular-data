{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"dl4td-chapter-6-code.ipynb","provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# *Modern Deep Learning for Tabular Data*, Chapter 6\n","\n","**Applying Attention to Tabular Data**\n","\n","This notebook contains the complementary code discussed in Chapter 6 of *Modern Deep Learning for Tabular Data*.\n","\n","External Kaggle links to datasets used in this notebook:\n","- [TripAdvisor Hotel Reviews](https://www.kaggle.com/datasets/andrewmvd/trip-advisor-hotel-reviews)\n","- [Daily Reddit News for Stock Market Prediction](https://www.kaggle.com/datasets/aaron7sun/stocknews)\n","- [Forest Cover Type Dataset](https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset)\n","\n","You can download these datasets from Kaggle, or import these notebooks into Kaggle and connect them internally."],"metadata":{"id":"6FqXzguj_o2F"}},{"cell_type":"code","source":["# data management\n","import numpy as np                   # for linear algebra\n","import pandas as pd                  # for tabular data manipulation and processing\n","\n","# machine learning\n","import sklearn                       # for data prep and classical ML\n","import tensorflow as tf              # for deep learning\n","from tensorflow import keras         # for deep learning\n","import keras.layers as L             # for easy NN layer access\n","from keras import backend as K       # for accessing Keras backend\n","\n","# data visualization and graphics\n","import matplotlib.pyplot as plt      # for visualization fundamentals\n","import seaborn as sns                # for pretty visualizations\n","import cv2                           # for image manipulation\n","\n","# misc\n","from tqdm.notebook import tqdm       # for progress bars\n","import math                          # for calculation\n","import sys                           # for system manipulation\n","import os                            # for file manipulation"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-05T04:26:33.944339Z","iopub.execute_input":"2022-07-05T04:26:33.945017Z","iopub.status.idle":"2022-07-05T04:26:45.268223Z","shell.execute_reply.started":"2022-07-05T04:26:33.944983Z","shell.execute_reply":"2022-07-05T04:26:45.266944Z"},"trusted":true,"id":"WH3jjMzt_o2K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"H8Z_ZSII_o2M"}},{"cell_type":"markdown","source":["## The Attention Mechanism in Keras"],"metadata":{"id":"m2XEiTyt_o2N"}},{"cell_type":"markdown","source":["Custom attention layer."],"metadata":{"id":"7Xy25QP9_o2N"}},{"cell_type":"code","source":["class Attention(keras.layers.Layer):\n","    def __init__(self,**kwargs):\n","        super(Attention,self).__init__(**kwargs)\n"," \n","    def build(self,input_shape):\n","        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n","                               initializer='random_normal', trainable=True)\n","        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n","                               initializer='zeros', trainable=True)        \n","        super(Attention, self).build(input_shape)\n"," \n","    def call(self,x):\n","        context = x * self.get_alpha(x)\n","        context = K.sum(context, axis=1)\n","        return context\n","    \n","    def get_alpha(self,x):\n","        e = K.tanh(K.dot(x, self.W)+self.b)\n","        e = K.squeeze(e, axis=-1)\n","        alpha = K.softmax(e)\n","        alpha = K.expand_dims(alpha, axis=-1)\n","        return alpha"],"metadata":{"execution":{"iopub.status.busy":"2022-07-05T04:26:45.270994Z","iopub.execute_input":"2022-07-05T04:26:45.271948Z","iopub.status.idle":"2022-07-05T04:26:45.776981Z","shell.execute_reply.started":"2022-07-05T04:26:45.2719Z","shell.execute_reply":"2022-07-05T04:26:45.775875Z"},"trusted":true,"id":"E_vU_AyW_o2O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Demonstrating the attention scores of the custom attention layer."],"metadata":{"id":"xjgGbfnd_o2O"}},{"cell_type":"code","source":["x, y = [], []\n","\n","NUM_SAMPLES = 10_000\n","\n","next_element = lambda arr: arr[-2] + arr[-4]\n","\n","vector_switch = [np.zeros((1,8)), np.ones((1,8))]\n","for i in tqdm(range(NUM_SAMPLES)):\n","    seed = np.random.normal(0, 5, size=(10,8))\n","    x.append(seed)\n","    y.append(next_element(seed))\n","    \n","x = np.array(x)\n","y = np.array(y)\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(x, y, train_size=0.8)"],"metadata":{"execution":{"iopub.status.busy":"2022-07-05T04:26:45.778459Z","iopub.execute_input":"2022-07-05T04:26:45.779445Z","iopub.status.idle":"2022-07-05T04:26:46.121781Z","shell.execute_reply.started":"2022-07-05T04:26:45.779346Z","shell.execute_reply":"2022-07-05T04:26:46.12041Z"},"trusted":true,"id":"S2ISAmXw_o2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp = L.Input((10,8))\n","lstm1 = L.GRU(16, return_sequences=True)(inp)\n","lstm2 = L.GRU(16, return_sequences=True)(lstm1)\n","attention = Attention()\n","attended = attention(lstm2)\n","dense = L.Dense(16, activation='relu')(attended)\n","dense2 = L.Dense(16, activation='relu')(dense)\n","out = L.Dense(8, activation='linear')(dense2)\n","\n","model = keras.models.Model(inputs=inp, outputs=out)\n","\n","model.compile(optimizer='adam', \n","              loss='mse',\n","              metrics=['mae'])\n","model.fit(X_train, y_train,\n","          validation_data=(X_valid, y_valid),\n","          epochs=10)"],"metadata":{"execution":{"iopub.status.busy":"2022-07-05T04:28:44.482598Z","iopub.execute_input":"2022-07-05T04:28:44.483339Z","iopub.status.idle":"2022-07-05T04:29:28.506073Z","shell.execute_reply.started":"2022-07-05T04:28:44.483297Z","shell.execute_reply":"2022-07-05T04:29:28.504783Z"},"trusted":true,"id":"rISLS-be_o2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp = L.Input((10,8))\n","rnn1 = model.layers[1](inp)\n","rnn2 = model.layers[2](rnn1)\n","submodel = keras.models.Model(inputs=inp, outputs=rnn2)\n","\n","recurrent_out = tensorflow.constant(submodel.predict(x))\n","\n","plt.figure(figsize=(10, 5), dpi=400)\n","plt.bar(range(10), attention.get_alpha(recurrent_out[0,:,0], \n","        color='red')\n","plt.ylabel('Alpha Values')\n","plt.xlabel('Time Step')\n","plt.show()"],"metadata":{"id":"dYeJ5NIa_o2Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Demonstrating Keras' native attention layers."],"metadata":{"id":"8Oy99tgo_o2R"}},{"cell_type":"code","source":["sigmoid = lambda x: 1/(1 + np.exp(-x))\n","sigmoid_deriv = lambda x: sigmoid(x) * sigmoid(-x)\n","adjusted_sigmoid_deriv = lambda x: 4 * sigmoid_deriv(x - 5)\n","weights = adjusted_sigmoid_deriv(np.linspace(0, 10, 10))\n","\n","x, y = [], []\n","\n","NUM_SAMPLES = 10_000\n","\n","next_element = lambda arr: np.dot(weights, arr)\n","\n","for i in tqdm(range(NUM_SAMPLES)):\n","    seed = np.random.normal(0, 1, size=(10,8))\n","    x.append(seed)\n","    y.append(next_element(seed))\n","    \n","x = np.array(x)\n","y = np.array(y)\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(x, y, train_size=0.8)"],"metadata":{"execution":{"iopub.status.busy":"2022-07-05T04:30:15.782527Z","iopub.execute_input":"2022-07-05T04:30:15.7829Z","iopub.status.idle":"2022-07-05T04:30:16.00868Z","shell.execute_reply.started":"2022-07-05T04:30:15.782862Z","shell.execute_reply":"2022-07-05T04:30:16.007204Z"},"trusted":true,"id":"kGM7f1Jv_o2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp = L.Input((10,8))\n","lstm1 = L.Bidirectional(L.LSTM(8, return_sequences=True))(inp)\n","attended = L.Attention(use_scale=True)([lstm1, lstm1])\n","lstm2 = L.LSTM(16)(attended)\n","dense = L.Dense(16, activation='relu')(lstm2)\n","dense2 = L.Dense(16, activation='relu')(dense)\n","out = L.Dense(8, activation='linear')(dense2)\n","\n","model = keras.models.Model(inputs=inp, outputs=out)\n","\n","model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","history = model.fit(X_train, y_train, epochs=1000,\n","                    validation_data=(X_valid, y_valid))"],"metadata":{"id":"o7V6P7Uv_o2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lstm1_ = model.layers[1](inp)\n","_, attn = model.layers[2]([lstm1_, lstm1_], \n","                          return_attention_scores=True)\n","submodel = keras.models.Model(inputs=inp, outputs=attn)\n","\n","scores = submodel.predict(X_train)"],"metadata":{"id":"lcn3m0DB_o2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,12), dpi=400)\n","sns.heatmap(scores[0,:,:], cbar=False)\n","plt.show()"],"metadata":{"id":"s9ciUj8d_o2T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Demonstrating Keras' multi-head attention layer."],"metadata":{"id":"AVyAPGgG_o2T"}},{"cell_type":"code","source":["sigmoid = lambda x: 1/(1 + np.exp(-x))\n","sigmoid_deriv = lambda x: sigmoid(x) * sigmoid(-x)\n","adjusted_sigmoid_deriv1 = lambda x: 4 * sigmoid_deriv(x - 2)\n","adjusted_sigmoid_deriv2 = lambda x: 4 * sigmoid_deriv(x - 8)\n","x = np.linspace(0, 10, 10)\n","weights = adjusted_sigmoid_deriv1(x) + adjusted_sigmoid_deriv2(x)\n","\n","x, y = [], []\n","\n","NUM_SAMPLES = 10_000\n","\n","next_element = lambda arr: np.dot(weights, arr)\n","\n","for i in tqdm(range(NUM_SAMPLES)):\n","    seed = np.random.normal(0, 1, size=(10,8))\n","    x.append(seed)\n","    y.append(next_element(seed))\n","\n","x = np.array(x)\n","y = np.array(y)\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(x, y, train_size=0.8)"],"metadata":{"execution":{"iopub.status.busy":"2022-07-05T04:34:31.555421Z","iopub.execute_input":"2022-07-05T04:34:31.556471Z","iopub.status.idle":"2022-07-05T04:34:31.761368Z","shell.execute_reply.started":"2022-07-05T04:34:31.556422Z","shell.execute_reply":"2022-07-05T04:34:31.760113Z"},"trusted":true,"id":"yDY1Ezu6_o2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp = L.Input((10,8))\n","lstm1 = L.Bidirectional(L.LSTM(8, return_sequences=True))(inp)\n","attended, scores = L.MultiHeadAttention(num_heads=4, \n","                                        key_dim=16)(lstm1,\n","                                                    lstm1,\n","                                                    return_attention_scores=True)\n","lstm2 = L.LSTM(16)(attended)\n","dense = L.Dense(16, activation='relu')(lstm2)\n","dense2 = L.Dense(16, activation='relu')(dense)\n","out = L.Dense(8, activation='linear')(dense2)\n","\n","model = keras.models.Model(inputs=inp, outputs=out)\n","\n","model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","history = model.fit(X_train, y_train, epochs=1000,\n","                    validation_data=(X_valid, y_valid))"],"metadata":{"id":"anM5qO9a_o2U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(24,24), dpi=400)\n","\n","for i in range(2):\n","    for j in range(2):\n","        plt.subplot(2, 2, 2*i + j + 1)\n","        sns.heatmap(scores[0,2*i + j,:,:], cbar=False)\n","plt.show()"],"metadata":{"id":"2tLYhTAH_o2U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Demonstrating a sequence-to-sequence problem."],"metadata":{"id":"AqYh_dmC_o2V"}},{"cell_type":"code","source":["x, y = [], []\n","\n","NUM_SAMPLES = 10_000\n","\n","next_element = lambda arr: np.stack([arr[(i+4)%10] + arr[(i+5)%10] + arr[(i+6)%10] for i in range(10)])\n","\n","for i in tqdm(range(NUM_SAMPLES)):\n","    seed = np.random.normal(0, 5, size=(10,8))\n","    x.append(seed)\n","    y.append(next_element(seed))\n","    \n","x = np.array(x)\n","y = np.array(y)\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(x, y, train_size=0.8)"],"metadata":{"id":"NBSARRmX_o2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp = L.Input((10,8))\n","encoder = L.Bidirectional(L.LSTM(16, return_sequences=True))(inp)\n","encoder2 = L.LSTM(16, return_sequences=True)(encoder)\n","decoder = L.LSTM(16, return_sequences=True)(encoder2)\n","attn, scores = L.Attention(use_scale=True)([decoder, encoder2], \n","                                           return_attention_scores=True)\n","concat = L.Concatenate()([decoder, attn])\n","out = L.TimeDistributed(L.Dense(8, activation='linear'))(concat)\n","\n","model = keras.models.Model(inputs=inp, outputs=out)\n","\n","model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","history = model.fit(X_train, y_train, epochs=1000,\n","                    validation_data=(X_valid, y_valid))"],"metadata":{"id":"Y1Ki1x-o_o2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submodel = keras.models.Model(inputs=inp, outputs=scores)\n","scores = submodel.predict(X_train)\n","\n","for i in range(4):    \n","    plt.figure(figsize=(12,12), dpi=400)\n","    sns.heatmap(scores[i,:,:], cbar=False)\n","    plt.show()"],"metadata":{"id":"f513e9CP_o2W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"tVmaYvjd_o2W"}},{"cell_type":"markdown","source":["## Improving Natural Language Models with Attention"],"metadata":{"id":"CQxvGdiq_o2W"}},{"cell_type":"markdown","source":["### TripAdvisor dataset modeling."],"metadata":{"id":"yJM7Ky8C_o2W"}},{"cell_type":"code","source":["data = pd.read_csv('../input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')\n","data.head()"],"metadata":{"execution":{"iopub.status.busy":"2022-07-05T04:39:59.441831Z","iopub.execute_input":"2022-07-05T04:39:59.442252Z","iopub.status.idle":"2022-07-05T04:39:59.86032Z","shell.execute_reply.started":"2022-07-05T04:39:59.442217Z","shell.execute_reply":"2022-07-05T04:39:59.859094Z"},"trusted":true,"id":"cCqSlhyC_o2W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEQ_LEN, MAX_TOKENS = 128, 2048\n","EMBEDDING_DIM = 16\n","\n","vectorize = tensorflow.keras.layers.TextVectorization(max_tokens=MAX_TOKENS,\n","                                                      output_sequence_length=SEQ_LEN)\n","vectorize.adapt(data['Review'])"],"metadata":{"id":"lPTIX-Nz_o2X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = data['Rating'] - 1\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(data['Review'], labels, train_size=0.8)\n","\n","X_train_vec = vectorize(X_train)\n","X_valid_vec = vectorize(X_valid)"],"metadata":{"id":"Ab6R-LeC_o2X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Custom attention layer."],"metadata":{"id":"Ojor9ts1_o2Y"}},{"cell_type":"code","source":["inp = L.Input((SEQ_LEN,))\n","embed = L.Embedding(MAX_TOKENS, EMBEDDING_DIM)(inp)\n","rnn1 = L.LSTM(16, return_sequences=True)(embed)\n","rnn2 = L.LSTM(16, return_sequences=True)(rnn1)\n","attn = Attention()(rnn2)\n","dense = L.Dense(16, activation='relu')(attn)\n","dense2 = L.Dense(16, activation='relu')(dense)\n","out = L.Dense(5, activation='softmax')(dense2)\n","\n","model = keras.models.Model(inputs=inp, outputs=out)"],"metadata":{"id":"uGCrHp-X_o2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","history = model.fit(X_train_vec, y_train, epochs=200,\n","                    validation_data=(X_valid_vec, y_valid))"],"metadata":{"id":"LlBlESTA_o2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp = L.Input((SEQ_LEN,))\n","embed = model.layers[1](inp)\n","rnn1 = model.layers[2](embed)\n","rnn2 = model.layers[3](rnn1)\n","submodel = keras.models.Model(inputs=inp, outputs=rnn2)\n","\n","for index in range(5):\n","    \n","    fig, ax = plt.subplots(figsize=(10, 5), dpi=400)\n","    lstm_encodings = tensorflow.constant(submodel.predict(X_train_vec[index:index+1]))\n","    alpha_values = model.layers[4].get_alpha(lstm_encodings)[0,:,0]\n","    bars = ax.bar(range(SEQ_LEN), alpha_values, color='red', alpha=0.7)\n","    text = X_train[X_train.index[index]].split(' ')\n","    text += ['']*(SEQ_LEN - len(text))\n","    for i, bar in enumerate(bars):\n","        height = bar.get_height()\n","        ax.text(x=bar.get_x() + bar.get_width() / 2 - 0.02, y=height+.0002,\n","                rotation = 90, size=6,\n","                s=text[i],\n","                ha='center')\n","    ax.set_ylabel('Alpha Values')\n","    ax.set_xlabel('Time Step')\n","    ax.axes.yaxis.set_visible(False)\n","    plt.show()"],"metadata":{"id":"A5PPPzYO_o2Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keras attention layer."],"metadata":{"id":"vpYLwYWN_o2Z"}},{"cell_type":"code","source":["inp = L.Input((SEQ_LEN,))\n","embed = L.Embedding(MAX_TOKENS, EMBEDDING_DIM)(inp)\n","rnn1 = L.Bidirectional(L.GRU(16, return_sequences=True))(embed)\n","attn, scores = L.MultiHeadAttention(num_heads=4, key_dim=4)(rnn1, rnn1,\n","                                    return_attention_scores=True)\n","rnn2 = L.LSTM(16, return_sequences=True)(attn)\n","rnn3 = L.LSTM(16)(rnn2)\n","dense = L.Dense(8, activation='relu')(rnn3)\n","dense2 = L.Dense(8, activation='relu')(dense)\n","out = L.Dense(5, activation='softmax')(dense2)\n","\n","model = keras.models.Model(inputs=inp, outputs=out)"],"metadata":{"id":"VQ6499Nt_o2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","history = model.fit(X_train_vec, y_train, epochs=200,\n","                    validation_data=(X_valid_vec, y_valid))"],"metadata":{"id":"_5D3WGAw_o2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submodel = keras.models.Model(inputs=inp, outputs=scores)\n","attn_scores = submodel.predict(X_train_vec)\n","vocab = vectorize.get_vocabulary()"],"metadata":{"id":"c7YH3uRU_o2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index in range(5):\n","\n","    vectorized_text = X_train_vec[index]\n","    words = [vocab[token] for token in vectorized_text.numpy()]\n","\n","    plt.figure(figsize=(12,12), dpi=400)\n","    sns.heatmap(attn_scores[index,0,:,:], cbar=False,\n","                xticklabels=words, yticklabels=words)\n","    plt.show()"],"metadata":{"id":"09yCyNTf_o2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reddit News and Stock Forecasting"],"metadata":{"id":"u1QI2CIA_o2a"}},{"cell_type":"code","source":["news = pd.read_csv('../input/stocknews/Combined_News_DJIA.csv')\n","news = news[['Top1', 'Top2', 'Top3', 'Date']]\n","stock = pd.read_csv('../input/stocknews/upload_DJIA_table.csv')\n","data = news.merge(stock, how='inner', left_on='Date', right_on='Date')\n","stock = data[['Open', 'High', 'Low', 'Close']]\n","stock /= 1000"],"metadata":{"execution":{"iopub.status.busy":"2022-07-05T04:52:14.664924Z","iopub.execute_input":"2022-07-05T04:52:14.665502Z","iopub.status.idle":"2022-07-05T04:52:14.914507Z","shell.execute_reply.started":"2022-07-05T04:52:14.66546Z","shell.execute_reply":"2022-07-05T04:52:14.913132Z"},"trusted":true,"id":"-3At7WKA_o2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["WINDOW_LENGTH = 20\n","\n","x_stock = np.zeros((len(stock) - WINDOW_LENGTH,\n","                    WINDOW_LENGTH,\n","                    len(stock.columns)))\n","y_stock = np.zeros((len(stock) - WINDOW_LENGTH,\n","                    len(stock.columns)))\n","\n","for i in range(len(stock) - WINDOW_LENGTH):\n","    x_stock[i] = np.array(stock.loc[i:i+WINDOW_LENGTH-1])\n","    y_stock[i] = np.array(stock.loc[i+WINDOW_LENGTH])"],"metadata":{"id":"C70aY-of_o2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data.loc[WINDOW_LENGTH:]\n","top1_text, top2_text, top3_text = data['Top1'], data['Top2'], data['Top3']"],"metadata":{"id":"1mDPfpvY_o2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEQ_LEN, MAX_TOKENS = 32, 10_000\n","EMBEDDING_DIM = 32\n","\n","vectorize = tensorflow.keras.layers.TextVectorization(max_tokens=MAX_TOKENS,\n","                                                      output_sequence_length=SEQ_LEN)\n","vectorize.adapt(pd.concat([top1_text, top2_text, top3_text]))"],"metadata":{"id":"XjHQGJiC_o2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top1_text = vectorize(top1_text)\n","top2_text = vectorize(top2_text)\n","top3_text = vectorize(top3_text)"],"metadata":{"id":"RVGB45Fx_o2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["variables = ['x_stock', 'y_stock', \n","             'top1_text', 'top2_text', 'top3_text']\n","\n","train_prop = 0.8\n","train_index = round(train_prop * len(data))\n","for variable in variables:\n","    exec(f'{variable}_train = {variable}[:{train_index}]')\n","    exec(f'{variable}_valid = {variable}[{train_index}:]')"],"metadata":{"id":"jp92ObTr_o2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top1_inp = L.Input((SEQ_LEN,), name='top1')\n","top2_inp = L.Input((SEQ_LEN,), name='top2')\n","top3_inp = L.Input((SEQ_LEN,), name='top3')\n","\n","embed = L.Embedding(MAX_TOKENS, EMBEDDING_DIM)\n","top1_embed = embed(top1_inp)\n","top2_embed = embed(top2_inp)\n","top3_embed = embed(top3_inp)\n","\n","lstm1 = L.Bidirectional(L.LSTM(16, return_sequences=True))\n","top1_lstm1 = lstm1(top1_embed)\n","top2_lstm1 = lstm1(top2_embed)\n","top3_lstm1 = lstm1(top3_embed)\n","\n","attn = L.MultiHeadAttention(num_heads=3, key_dim=4,\n","                            dropout=0.1)\n","\n","top1_lstm2 = L.LSTM(32)(attn(top1_lstm1, top1_lstm1))\n","top2_lstm2 = L.LSTM(32)(attn(top2_lstm1, top2_lstm1))\n","top3_lstm2 = L.LSTM(32)(attn(top3_lstm1, top3_lstm1))\n","\n","concat = L.Concatenate()([top1_lstm2, top2_lstm2, top3_lstm2])\n","concat_dense = L.Dense(16, activation='relu')(concat)\n","\n","stock_inp = L.Input((WINDOW_LENGTH, 4), name='stock')\n","stock_cnn1 = L.Conv1D(8, 5, activation='relu')(stock_inp)\n","stock_lstm1 = L.LSTM(8, return_sequences=True)(stock_cnn1)\n","stock_lstm2 = L.LSTM(8)(stock_lstm1)\n","\n","joint_concat = L.Concatenate()([concat_dense, stock_lstm2])\n","joint_dense1 = L.Dense(16, activation='relu')(joint_concat)\n","joint_dense2 = L.Dense(16, activation='relu')(joint_dense1)\n","out = L.Dense(4, activation='relu')(joint_dense2)\n","\n","model = keras.models.Model(inputs={'top1': top1_inp,\n","                                   'top2': top2_inp,\n","                                   'top3': top3_inp,\n","                                   'stock': stock_inp},\n","                          outputs=out)"],"metadata":{"id":"3q7PwLB5_o2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","\n","history = model.fit(x={'top1': top1_text_train,\n","             'top2': top2_text_train,\n","             'top3': top3_text_train,\n","             'stock': x_stock_train},\n","          y=y_stock_train,\n","          validation_data=({'top1': top1_text_valid,\n","                            'top2': top2_text_valid,\n","                            'top3': top3_text_valid,\n","                            'stock': x_stock_valid},\n","                            y_stock_valid),\n","         batch_size=128,\n","         epochs=300)"],"metadata":{"id":"5NILh4xw_o2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15, 7), dpi=400)\n","plt.plot(history.history['loss'], color='red', label='Train')\n","plt.plot(history.history['val_loss'], color='blue', label='Validation')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"c1Fjhn6J_o2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, attn_top1 = attn(top1_lstm1, top1_lstm1, return_attention_scores=True)\n","_, attn_top2 = attn(top2_lstm1, top2_lstm1, return_attention_scores=True)\n","_, attn_top3 = attn(top3_lstm1, top3_lstm1, return_attention_scores=True)\n","\n","submodel = keras.models.Model(inputs=[top1_inp, top2_inp, top3_inp], outputs=[attn_top1, attn_top2, attn_top3])"],"metadata":{"id":"5zsbvWpD_o2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = vectorize.get_vocabulary()"],"metadata":{"id":"w3--Mfeq_o2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index in range(5):\n","    \n","    scores = submodel.predict([top1_text_train[index:index+1],\n","                               top2_text_train[index:index+1],\n","                               top3_text_train[index:index+1]])\n","    top1_scores, top2_scores, top3_scores = scores\n","\n","    bundles = ((top1_scores, top1_text_train[index]),\n","               (top2_scores, top2_text_train[index]),\n","               (top3_scores, top3_text_train[index]))\n","\n","    for curr_scores, vectorized_text in bundles:\n","\n","        words = [vocab[token] for token in vectorized_text.numpy()]\n","        if sum([1 for i in words if i != 0]) > 25:\n","        \n","            print('-'* 500)\n","            print(''.join(words))\n","\n","            for i in range(len(curr_scores[0])): # number heads\n","                plt.figure(figsize=(12,12), dpi=400)\n","                sns.heatmap(curr_scores[0,i,:,:], cbar=False,\n","                            xticklabels=words, yticklabels=words)\n","                plt.show()"],"metadata":{"id":"0uA6L7lt_o2c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"nRoT35Bm_o2c"}},{"cell_type":"markdown","source":["## Direct Attention Modeling"],"metadata":{"id":"FuTFkOK9_o2c"}},{"cell_type":"code","source":["data = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n","X = data.drop('Cover_Type', axis=1)\n","y = data['Cover_Type'] - 1\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(X, y, train_size=0.8)"],"metadata":{"id":"AyKCPHkL_o2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def attn_block(inp, \n","               dense_units=8,\n","               num_heads=4,\n","               key_dim=4):\n","    dense = L.Dense(dense_units, activation='relu')(inp)\n","    dense2 = L.Dense(dense_units, activation='relu')(dense)\n","    attn_out = L.Attention(use_scale=True)([dense2, dense2])\n","    layer_norm = L.LayerNormalization()(attn_out)\n","    return layer_norm\n","\n","inp = L.Input((len(X_train.columns),))\n","reshape = L.Reshape((len(X_train.columns),1))(inp)\n","attn1 = attn_block(reshape)\n","attn2 = attn_block(attn1)\n","flatten = L.Flatten()(attn2)\n","predense = L.Dense(32, activation='relu')(flatten)\n","out = L.Dense(7, activation='softmax')(predense)\n","\n","model = keras.models.Model(inputs=inp, outputs=out)"],"metadata":{"id":"wteyvkM7_o2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam', \n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=100,\n","          batch_size=4096,\n","          validation_data=(X_valid, y_valid))"],"metadata":{"id":"FMf9qVFu_o2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"aL5Iy5Uu_o2d"}},{"cell_type":"markdown","source":["## Attention-Based Deep Learning Research"],"metadata":{"id":"JFVhLsSd_o2d"}},{"cell_type":"markdown","source":["### TabTransformer"],"metadata":{"id":"ynZ73ZUp_o2d"}},{"cell_type":"markdown","source":["Loading data."],"metadata":{"id":"ecg_PH2c_o2d"}},{"cell_type":"code","source":["df = pd.read_csv('https://raw.githubusercontent.com/hjhuney/Data/master/AmesHousing/train.csv')\n","df = df.dropna(axis=1, how='any').drop('Id', axis=1)\n","x = df.drop('SalePrice', axis=1)\n","y = df['SalePrice'] / 1000\n","\n","cat_features = []\n","for colIndex, colName in enumerate(x.columns):\n","    if type(x.iloc[0, colIndex]) == str or len(x[colName].unique()) <= 5:\n","        cat_features.append(colName)\n","cont_features = [col for col in x.columns if col not in cat_features]\n","\n","from sklearn.preprocessing import OrdinalEncoder\n","encoders = {col:OrdinalEncoder() for col in cat_features}\n","for cat_feature in cat_features:\n","    encoder = encoders[cat_feature]\n","    x[cat_feature] = encoder.fit_transform(np.array(x[cat_feature]).reshape(-1, 1)) #.astype(np.float32)\n","    \n","for cont_feature in cont_features:\n","    x[cont_feature] = x[cont_feature].astype(np.float32)\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(x, y, train_size=0.8)"],"metadata":{"id":"vZij0ddD_o2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Custom architecture."],"metadata":{"id":"EdtqdKjO_o2d"}},{"cell_type":"code","source":["'''\n","CONFIG\n","'''\n","\n","NUM_CONT_FEATS = 8\n","NUM_CAT_FEATS = 4\n","NUM_UNIQUE_CLASSES = [32 for i in range(NUM_CAT_FEATS)]\n","\n","EMBEDDING_DIM = 32\n","\n","NUM_HEADS = 4\n","KEY_DIM = 4\n","NUM_TRANSFORMERS = 6\n","FF_HIDDEN_DIM = 32\n","\n","MLP_LAYERS = 4\n","MLP_HIDDEN = 16\n","\n","OUT_DIM = 1\n","OUT_ACTIVATION = 'linear'\n","\n","'''\n","ARCHITECTURE\n","'''\n","\n","cont_inp = L.Input((NUM_CONT_FEATS,), name='Cont Feats')\n","normalize = L.LayerNormalization()(cont_inp)\n","\n","cat_inps = [L.Input((1,),\n","                    name=f'Cat Feats {i}') for i in range(NUM_CAT_FEATS)]\n","zipped = zip(NUM_UNIQUE_CLASSES, cat_inps)\n","embeddings = [L.Embedding(uqcls, EMBEDDING_DIM)(cat_inp) for uqcls, cat_inp in zipped]\n","concat_embed = L.Concatenate(axis=1)(embeddings)\n","\n","def transformer(inp):\n","    attention = L.MultiHeadAttention(num_heads=NUM_HEADS,\n","                                     key_dim=KEY_DIM)(inp, inp)\n","    add = L.Add()([inp, attention])\n","    norm = L.LayerNormalization()(add)\n","    dense1 = L.Dense(FF_HIDDEN_DIM, activation='relu')(norm)\n","    dense2 = L.Dense(EMBEDDING_DIM, activation='relu')(dense1)\n","    add2 = L.Add()([norm, dense2])\n","    norm2 = L.LayerNormalization()(add2)\n","    return norm2\n","\n","transformed = concat_embed\n","for i in range(NUM_TRANSFORMERS):\n","    transformed = transformer(transformed)\n","contextual_embeddings = L.Flatten()(transformed)\n","\n","all_feat_concat = L.Concatenate()([normalize, contextual_embeddings])\n","mlp = all_feat_concat\n","for i in range(MLP_LAYERS):\n","    mlp = L.Dense(MLP_HIDDEN, activation='relu')(mlp)\n","out = L.Dense(OUT_DIM, activation=OUT_ACTIVATION)(mlp)\n","\n","all_inps = cat_inps + [cont_inp]\n","model = keras.models.Model(inputs=all_inps, outputs=out)"],"metadata":{"id":"74e_q6I0_o2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using prebuilt architecture."],"metadata":{"id":"XyI_ud-R_o2e"}},{"cell_type":"code","source":["!git clone https://github.com/CahidArda/tab-transformer-keras.git"],"metadata":{"id":"rCqr6g2v_o2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.rename('./tab-transformer-keras', './tab_transformer_keras')"],"metadata":{"id":"kms7d8U__o2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tab_transformer_keras.tab_transformer_keras import tab_transformer_keras\n","from tab_transformer_keras import misc"],"metadata":{"id":"3x-uiFJw_o2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tab_transformer_keras.tab_transformer_keras.tab_transformer_keras import TabTransformer\n","from tab_transformer_keras.misc import get_X_from_features\n","\n","X_train_tt = get_X_from_features(X_train, cont_features, cat_features)\n","X_valid_tt = get_X_from_features(X_valid, cont_features, cat_features)\n","class_counts = [x[col].nunique() for col in cat_features]\n","model = TabTransformer(\n","    categories = class_counts,\n","    num_continuous = len(cat_features),\n","    dim = 16,\n","    dim_out = 1,\n","    depth = 6,\n","    heads = 8,\n","    attn_dropout = 0.1,\n","    ff_dropout = 0.1,\n","    mlp_hidden = [(32, 'relu'), (16, 'relu')] # mlp layer dimensions and activations\n",")"],"metadata":{"id":"H2kg4-IK_o2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","history = model.fit(X_train_tt, y_train, epochs=500,\n","                    validation_data=(X_valid_tt, y_valid))"],"metadata":{"id":"ncSdJy_B_o2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 5), dpi=400)\n","plt.plot(history.history['loss'], color='red', label='Training')\n","plt.plot(history.history['val_loss'], color='blue', label=['Validation'],\n","         linestyle='--')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"eM1oyNYe_o2f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TabNet"],"metadata":{"id":"VmG5pqTm_o2f"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split as tts\n","data = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n","X, y = data.drop('Cover_Type', axis=1), data['Cover_Type'] - 1\n","columns = X.columns\n","X = np.array(X).astype(np.float32)\n","y = np.array(y).astype(np.float32)\n","X_train, X_valid, y_train, y_valid = tts(X, y, train_size=0.8)"],"metadata":{"id":"dQ7q6-oY_o2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade tabnet"],"metadata":{"id":"A0emrzsY_o2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tabnet import TabNetClassifier\n","model = TabNetClassifier(feature_columns=None,\n","                         num_classes=7,\n","                         num_features=X.shape[-1],\n","                         feature_dim=32,\n","                         output_dim=16,\n","                         num_decision_steps=8,\n","                         relaxation_factor=0.7,\n","                         sparsity_coefficient=1e-6)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=30,\n","          validation_data=(X_valid, y_valid),\n","          batch_size=10_000)"],"metadata":{"id":"OMpD0G0l_o2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_ = model(X_valid)\n","fs_masks_orig = model.tabnet.feature_selection_masks\n","fs_masks = np.stack([mask.numpy()[0,:,:,0] for mask in fs_masks_orig])\n","\n","for i in range(7):\n","    plt.figure(figsize=(15, 8), dpi=400)\n","    sns.heatmap(fs_masks[i,:100,:],\n","                xticklabels=columns,\n","                yticklabels=[])\n","    plt.xlabel('Columns')\n","    plt.ylabel('Samples')\n","    plt.title(f'Sample of Mask Values for Layer {i+1}')\n","    plt.show()"],"metadata":{"id":"cmZK4hn1_o2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agg_mask = model.tabnet.aggregate_feature_selection_mask\n","\n","plt.figure(figsize=(15, 8), dpi=400)\n","sns.heatmap(agg_mask.numpy()[0,:100,:,0],\n","            xticklabels=columns,\n","            yticklabels=[])\n","plt.xlabel('Columns')\n","plt.ylabel('Samples')\n","plt.title(f'Aggregate Feature Mask')\n","plt.show()"],"metadata":{"id":"bh3E9lys_o2f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SAINT\n","\n","See book for details."],"metadata":{"id":"zv-JLaCd_o2g"}},{"cell_type":"markdown","source":["### ARM-Net\n","\n","See book for details."],"metadata":{"id":"p4RyJMXx_o2g"}}]}