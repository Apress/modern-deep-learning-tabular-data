{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"dl4td-chapter-11.ipynb","provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# *Modern Deep Learning for Tabular Data*, Chapter 11\n","\n","**Multi-Model Arrangement**\n","\n","This notebook contains the complementary code discussed in Chapter 8 of *Modern Deep Learning for Tabular Data*.\n","\n","External Kaggle links to datasets used in this notebook:\n","- [Wildfire Satellite Data](https://www.kaggle.com/datasets/washingtongold/wildfire-satellite-data)\n","\n","You can download these datasets from Kaggle, or import these notebooks into Kaggle and connect them internally."],"metadata":{"id":"cM3CNLPaK3A9"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"ksa1x-Tvhepj"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"AGw_VUAShfcs"}},{"cell_type":"code","source":["# data management\n","import numpy as np                   # for linear algebra\n","import pandas as pd                  # for tabular data manipulation and processing\n","\n","# machine learning\n","import sklearn                       # for data prep and classical ML\n","import tensorflow\n","import tensorflow as tf              # for deep learning\n","from tensorflow import keras         # for deep learning\n","\n","# data visualization and graphics\n","import matplotlib.pyplot as plt      # for visualization fundamentals\n","import seaborn as sns                # for pretty visualizations\n","import cv2                           # for image manipulation\n","\n","# misc\n","from tqdm.notebook import tqdm       # for progress bars\n","import math                          # for calculation\n","import sys                           # for system manipulation\n","import os                            # for file manipulation\n","import hyperopt                      # for meta-optimization"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"tBTuM5SvK3A_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"HSVAyAvzK3BA"}},{"cell_type":"markdown","source":["## Preparation"],"metadata":{"id":"xitSGcF8K3BD"}},{"cell_type":"markdown","source":["Loading the data."],"metadata":{"id":"38E8mI1RgPbG"}},{"cell_type":"code","source":["data = pd.read_csv('../input/wildfire-satellite-data/fire_archive_M6_156000.csv')[:100_000]\n","data.drop(['acq_date', 'acq_time', 'instrument'], axis=1, inplace=True)\n","data['satellite'] = data['satellite'].map({'Terra':0, 'Aqua':1})\n","data['daynight'] = data['daynight'].map({'N':0, 'D':1})\n","X = data.drop('type', axis=1)\n","y = data['type']\n","\n","from sklearn.model_selection import train_test_split as tts\n","X_train, X_valid, y_train, y_valid = tts(X, y, train_size = 0.8, random_state = 42)"],"metadata":{"id":"O5E_kU-nK3BE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preparing sample learners."],"metadata":{"id":"yo0gKV48gRRs"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.neural_network import MLPClassifier\n","models = {'lr': LogisticRegression(),\n","          'dtc': DecisionTreeClassifier(),\n","          'rfc': RandomForestClassifier(),\n","          'gbc': GradientBoostingClassifier(),\n","          'abc': AdaBoostClassifier(),\n","          'mlpc': MLPClassifier()}\n","\n","for model in models:\n","    print(f'Fitting {model}')\n","    models[model].fit(X_train, y_train)\n","from sklearn.metrics import f1_score, accuracy_score\n","for model in models:\n","    print(model)\n","    print(accuracy_score(models[model].predict(X_valid), y_valid))\n","    print(f1_score(models[model].predict(X_valid), y_valid, average='macro'))"],"metadata":{"id":"FXf1kkwYK3BE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"uvDs8uqTgUsL"}},{"cell_type":"markdown","source":["## Average Ensembling"],"metadata":{"id":"FpMo4LouK3BE"}},{"cell_type":"code","source":["class AverageEnsemble:\n","    def __init__(self, modeldic):\n","        self.modeldic = modeldic\n","    def predict(self, x, num_classes = 4):\n","        votes = np.zeros((len(x), num_classes))\n","        for model in self.modeldic:\n","            predictions = self.modeldic[model].predict(x)\n","            for item, vote in enumerate(predictions):\n","                votes[item, vote] += 1\n","        return np.argmax(votes, axis=1)\n","    \n","ensemble = AverageEnsemble(models)\n","pred = ensemble.predict(X_train)\n","print(f1_score(pred, y_train, average='macro'))\n","print(accuracy_score(pred, y_train))"],"metadata":{"id":"mP223PbvK3BF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"nqTkCCZMgYcy"}},{"cell_type":"markdown","source":["## Weighted Average Ensembling"],"metadata":{"id":"G3Blwc-yK3BF"}},{"cell_type":"markdown","source":["Defining a weighted average ensemble model."],"metadata":{"id":"Z4m8XvjYhKtN"}},{"cell_type":"code","source":["class WeightedAverageEnsemble:\n","    def __init__(self, modeldic, modelweights):\n","        self.modeldic = modeldic\n","        self.modelweights = modelweights\n","    def predict(self, x, num_classes = 4):\n","        votes = np.zeros((len(x), num_classes))\n","        for model in self.modeldic:\n","            predictions = self.modeldic[model].predict(x)\n","            for item, vote in enumerate(predictions):\n","                votes[item, vote] += self.modelweights[model]\n","        return np.argmax(votes, axis=1)"],"metadata":{"id":"43QU6IQEhGlM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Deriving the optimal weights for classical machine learning learners."],"metadata":{"id":"z9bkOqj-gm8U"}},{"cell_type":"code","source":["from hyperopt import hp, tpe, fmin\n","\n","# define the search space\n","space = {model:hp.normal(model, mu = 1, sigma = 0.4) for model in models}\n","\n","# define objective function\n","def obj_func(params):\n","    ensemble = WeightedAverageEnsemble(models, params)\n","    return -f1_score(ensemble.predict(X_valid), y_valid,\n","                     average='macro')\n","\n","# perform minimization procedure\n","from hyperopt import fmin, tpe\n","best = fmin(obj_func, space, algo=tpe.suggest, max_evals=500)\n"],"metadata":{"id":"oapO9g64K3BF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Defining a family of neural network learners."],"metadata":{"id":"GfMd5j0QhR-N"}},{"cell_type":"code","source":["modelA = keras.models.Sequential(name='modelA')\n","modelA.add(L.Input((len(X_train.columns),)))\n","modelA.add(L.Dense(16, activation='relu'))\n","modelA.add(L.Dense(16, activation='relu'))\n","modelA.add(L.Dense(4, activation='softmax'))\n","\n","modelB = keras.models.Sequential(name='modelB')\n","modelB.add(L.Input((len(X_train.columns),)))\n","modelB.add(L.Dense(16, activation='relu'))\n","modelB.add(L.Dense(16, activation='relu'))\n","modelB.add(L.Dense(16, activation='relu'))\n","modelB.add(L.Dense(16, activation='relu'))\n","modelB.add(L.Dense(4, activation='softmax'))\n","\n","inp = L.Input((len(X_train.columns),))\n","dense = L.Dense(16, activation='relu')(inp)\n","branch1a = L.Dense(16, activation='relu')(dense)\n","branch1b = L.Dense(16, activation='relu')(branch1a)\n","branch1c = L.Dense(8, activation='relu')(branch1b)\n","branch2a = L.Dense(8, activation='relu')(dense)\n","branch2b = L.Dense(8, activation='relu')(branch2a)\n","concat = L.Concatenate()([branch1c, branch2b])\n","out = L.Dense(4, activation='softmax')(concat)\n","modelC = keras.models.Model(inputs=inp, outputs=out, name='modelC')\n","\n","modelD = keras.models.Sequential(name='modelD')\n","modelD.add(L.Input((len(X_train.columns),)))\n","modelD.add(L.Dense(64, activation='relu'))\n","modelD.add(L.Reshape((8, 8, 1)))\n","modelD.add(L.Conv2D(8, (3, 3), padding='same', activation='relu'))\n","modelD.add(L.Conv2D(8, (3, 3), padding='same', activation='relu'))\n","modelD.add(L.MaxPooling2D(2, 2))\n","modelD.add(L.Conv2D(16, (3, 3), padding='same', activation='relu'))\n","modelD.add(L.Conv2D(16, (3, 3), padding='same', activation='relu'))\n","modelD.add(L.Flatten())\n","modelD.add(L.Dense(16, activation='relu'))\n","modelD.add(L.Dense(4, activation='softmax'))\n","\n","modelE = keras.models.Sequential(name='modelE')\n","modelE.add(L.Input((len(X_train.columns),)))\n","modelE.add(L.Dense(64, activation='relu'))\n","modelE.add(L.Reshape((64, 1)))\n","modelE.add(L.Conv1D(8, 3, padding='same', activation='relu'))\n","modelE.add(L.Conv1D(8, 3, padding='same', activation='relu'))\n","modelE.add(L.MaxPooling1D(2))\n","modelE.add(L.Conv1D(16, 3, padding='same', activation='relu'))\n","modelE.add(L.Conv1D(16, 3, padding='same', activation='relu'))\n","modelE.add(L.Flatten())\n","modelE.add(L.Dense(16, activation='relu'))\n","modelE.add(L.Dense(4, activation='softmax'))\n","\n","models = {'modelA': modelA,\n","          'modelB': modelB,\n","          'modelC': modelC,\n","          'modelD': modelD,\n","          'modelE': modelE}\n","for model in models:\n","    tensorflow.keras.utils.plot_model(models[model], show_shapes=True, dpi=400, to_file=f'{model}.png')\n","    models[model].compile(optimizer='adam', \n","                          loss='sparse_categorical_crossentropy',\n","                          metrics='accuracy')\n","    models[model].fit(X_train, y_train, epochs=30)\n","    \n","for model in models:\n","    predictions = np.argmax(models[model].predict(X_valid), axis=1)\n","    f1 = f1_score(predictions, y_valid, average='macro')\n","    acc = accuracy_score(predictions, y_valid)\n","    print(f'{model}: F1 - {f1}, Acc - {acc}')\n","\n","\n","\n","class WeightedAverageEnsemble:\n","    def __init__(self, modeldic, modelweights):\n","        self.modeldic = modeldic\n","        self.modelweights = modelweights\n","    def predict(self, x, num_classes = 4):\n","        votes = np.zeros((len(x), num_classes))\n","        for model in self.modeldic:\n","            predictions = self.modeldic[model].predict(x)\n","            votes += self.modelweights[model] * predictions\n","        return np.argmax(votes, axis=1)\n","    \n","# define the search space\n","from hyperopt import hp\n","# space = {model:hp.normal(model, mu=1, sigma=0.5) for model in models}\n","space = {model:hp.normal(model, mu = 1, sigma = 0.75) for model in models}\n","\n","# define objective function\n","def obj_func(params):\n","    ensemble = WeightedAverageEnsemble(models, params)\n","    return -f1_score(ensemble.predict(X_valid), y_valid,\n","                     average='macro')\n","\n","# perform minimization procedure\n","from hyperopt import fmin, tpe\n","best = fmin(obj_func, space, algo=tpe.suggest, max_evals=500)"],"metadata":{"id":"QSbNHYA8K3BG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"pOXg9-UOhXkA"}},{"cell_type":"markdown","source":["## Input-Informed"],"metadata":{"id":"1PicRx1dK3BH"}},{"cell_type":"code","source":["for model in models:\n","    models[model].trainable = False\n","inp = L.Input((len(X_train.columns),))\n","mergeList = []\n","for model in models:\n","    modelOut = models[model](inp)\n","    reshape = L.Reshape((4, 1))(modelOut)\n","    scale = L.Conv1D(1, 1, activation='softmax')(reshape)\n","    flatten = L.Flatten()(scale)\n","    mergeList.append(flatten)\n","concat = L.Add()(mergeList)\n","scale = L.Lambda(lambda x: x/len(models))(concat)\n","metaModel = keras.models.Model(inputs=inp, outputs=scale)\n","\n","tensorflow.keras.utils.plot_model(metaModel, show_shapes=True, dpi=400)\n","\n","metaModel.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","metaModel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n","\n","metaModel.summary()\n","\n","f1_score(np.argmax(metaModel.predict(X_valid), axis=1),\n","         y_valid, \n","         average='macro')\n","for model in models:\n","    models[model].trainable = True\n","    \n","metaModel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"],"metadata":{"id":"NX9iNrcMK3BH"},"execution_count":null,"outputs":[]}]}