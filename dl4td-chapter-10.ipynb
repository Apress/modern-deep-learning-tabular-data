{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"dl4td-chapter-10.ipynb","provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# *Modern Deep Learning for Tabular Data*, Chapter 10\n","\n","**Meta-Optimization**\n","\n","This notebook contains the complementary code discussed in Chapter 10 of *Modern Deep Learning for Tabular Data*.\n","\n","External Kaggle links to datasets used in this notebook:\n","- [Forest Cover Type Dataset](https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset)\n","- [Mouse Protein Expression Dataset](https://www.kaggle.com/datasets/washingtongold/mpempe)\n","\n","You can download these datasets from Kaggle, or import these notebooks into Kaggle and connect them internally."],"metadata":{"id":"VTSvq_4CK2-C"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"jjLIGTDvcx-6"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"bxkvN0hXczWQ"}},{"cell_type":"code","source":["# installing packages\n","!pip install hyperopt\n","!pip install autokeras\n","\n","# data management\n","import numpy as np                   # for linear algebra\n","import pandas as pd                  # for tabular data manipulation and processing\n","from skimage import io               # for input/output processing\n","\n","# machine learning\n","import sklearn                       # for data prep and classical ML\n","import tensorflow as tf              # for deep learning\n","from tensorflow import keras         # for deep learning\n","import keras\n","import keras.layers as L             # for easy NN layer access\n","\n","# data visualization and graphics\n","import matplotlib.pyplot as plt      # for visualization fundamentals\n","import seaborn as sns                # for pretty visualizations\n","import cv2                           # for image manipulation\n","\n","# misc\n","from tqdm.notebook import tqdm       # for progress bars\n","import math                          # for calculation\n","import sys                           # for system manipulation\n","import os                            # for file manipulation\n","\n","# meta-optimization\n","import hyperopt                      # for parameter optimization\n","import autokeras as ak               # for AutoML"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"HS7vddGdK2-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"I5fL6wK3K2-I"}},{"cell_type":"markdown","source":["## HyperOpt Syntax"],"metadata":{"id":"JW_EVqIKK2-J"}},{"cell_type":"markdown","source":["Attempting to minimize $(x - 1)^2$:"],"metadata":{"id":"guVMAFOodAeX"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 5))\n","x = np.linspace(-5, 5, 100)\n","y = (x - 1)**2\n","plt.plot(x, y, color='red')\n","plt.scatter([1], [0], color='red')\n","\n","plt.grid()\n","plt.show()"],"metadata":{"id":"40kGbDNQK2-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the search space\n","from hyperopt import hp\n","space = {'x':hp.normal('x', mu=0, sigma=10)}\n","\n","# define objective function\n","def obj_func(params):\n","    return (params['x']-1)**2\n","\n","# perform minimization procedure\n","from hyperopt import fmin, tpe\n","best = fmin(obj_func, space, algo=tpe.suggest, max_evals=500)"],"metadata":{"id":"eLw8LXRxK2-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attempting to minimize $\\sin^2 x$:"],"metadata":{"id":"YA0fesj8K2-L"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 5), dpi=400)\n","x = np.linspace(2, 4, 1000)\n","y = np.sin(x)**2\n","plt.plot(x, y, color='red')\n","plt.scatter([np.pi], [0], color='red')\n","\n","x = np.linspace(-2, 8, 1000)\n","y = np.sin(x)**2\n","plt.plot(x, y, color='red', alpha=0.3, linestyle='--')\n","plt.scatter([np.pi], [0], color='red')\n","\n","plt.grid()\n","plt.show()"],"metadata":{"id":"DZRlJ3NwK2-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the search space\n","from hyperopt import hp\n","space = {'x':hp.uniform('x', 2, 4)}\n","\n","# define objective function\n","def obj_func(params):\n","    return np.sin(params['x'])**2\n","\n","# perform minimization procedure\n","from hyperopt import fmin, tpe\n","best = fmin(obj_func, space, algo=tpe.suggest, max_evals=10)\n","print(best)\n","\n","best = fmin(obj_func, space, algo=tpe.suggest, max_evals=100)\n","print(best)\n","\n","best = fmin(obj_func, space, algo=tpe.suggest, max_evals=1000)\n","print(best)"],"metadata":{"id":"eWpasAmzK2-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attempting to find the minimum of a very discontinuous function, $1/{x^2} + x^2$:"],"metadata":{"id":"bquavz5aK2-N"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 5), dpi=400)\n","x = np.linspace(-3.99214, -0.25049, 100)\n","y = 1/(x**2) + (x)**2\n","plt.plot(x, y, color='red')\n","\n","x = np.linspace(0.25049, 3.99214, 100)\n","y = 1/(x**2) + (x)**2\n","plt.plot(x, y, color='red')\n","plt.scatter([-1, 1], [2, 2], color='red')\n","\n","plt.grid()\n","plt.show()"],"metadata":{"id":"vfZ7PyDmK2-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the search space\n","from hyperopt import hp\n","space = {'x':hp.normal('x', mu=0, sigma=10)}\n","\n","# define objective function\n","def obj_func(params):\n","    if params['x']==0:\n","        return {'status':'fail'}\n","    return {'loss':1/(params['x']**2) + params['x']**2,\n","            'status':'ok'}\n","\n","# perform minimization procedure\n","from hyperopt import fmin, tpe\n","best = fmin(obj_func, space, algo=tpe.suggest, max_evals=500)"],"metadata":{"id":"6rXzCtYyK2-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best"],"metadata":{"id":"d1JtSPLHK2-P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finding the minimum of a 'very very discontinuous' function: $1/({\\sqrt{x} - \\sqrt{\\sin(x^2)}})$"],"metadata":{"id":"NSmIkolFK2-P"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 5), dpi=400)\n","x = np.linspace(0.1, 10, 10_000)\n","y = 1/(np.sqrt(x)) - np.sqrt(np.sin(x**2))\n","plt.plot(x, y, color='red')\n","\n","plt.grid()\n","plt.show()"],"metadata":{"id":"1KuM8mpNK2-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the search space\n","from hyperopt import hp\n","space = {'x':hp.uniform('x', -10, 10)}\n","\n","# define objective function\n","def obj_func(params):\n","    result = 1/np.sqrt(params['x']) - np.sqrt(np.sin(params['x']**2))\n","    if result == np.nan:\n","        return {'status':'fail'}\n","    return {'loss': result, 'status':'ok'}\n","\n","# perform minimization procedure\n","from hyperopt import fmin, tpe\n","best = fmin(obj_func, space, algo=tpe.suggest, max_evals=1000)"],"metadata":{"id":"Gwz0QEtUK2-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 5), dpi=400)\n","x = np.linspace(0.1, 10, 10_000)\n","y = 1/(np.sqrt(x)) - np.sqrt(np.sin(x**2))\n","plt.plot(x, y, color='red')\n","plt.scatter([best['x']], 1/(np.sqrt(best['x'])) - np.sqrt(np.sin(best['x']**2)), color='black')\n","plt.grid()\n","plt.show()"],"metadata":{"id":"CNGXCmd7K2-Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimizing a Model"],"metadata":{"id":"FDc6jFE9K2-Q"}},{"cell_type":"markdown","source":["Loading the Higgs Boson dataset."],"metadata":{"id":"CA5jlY2Ld8P-"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","train_data = pd.read_csv('../input/higs-bonsons-and-background-process/train.csv').drop('id', axis=1)\n","X_train = train_data.drop('class', axis=1)\n","y_train = train_data['class']\n","\n","valid_data = pd.read_csv('../input/higs-bonsons-and-background-process/test.csv').drop('id', axis=1).replace('?', np.nan).dropna()\n","X_valid = valid_data.drop('class', axis=1)\n","y_valid = valid_data['class']"],"metadata":{"id":"uhhFmOgwK2-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizing the (classical) machine learning algorithm and its parameters."],"metadata":{"id":"BDRQD9QMd_c2"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","from sklearn.metrics import f1_score\n","\n","from hyperopt import hp\n","from hyperopt import fmin, tpe\n","\n","space = {}\n","\n","models = [{'model': LogisticRegression,\n","           'parameters':{'penalty':hp.choice('lr_penalty', ['none', 'l2', 'l1', 'elasticnet'])}},\n","          {'model': DecisionTreeClassifier,\n","           'parameters': {'criterion': hp.choice('dtr_criterion', ['gini', 'entropy']),\n","                          'max_depth': hp.quniform('dtr_max_depth', 1, 30, 1)}},\n","          {'model': RandomForestClassifier,\n","           'parameters': {\n","               'criterion': hp.choice('rfr_criterion', ['gini', 'entropy']),\n","               'max_depth': hp.quniform('rfr_max_depth', 1, 30, q=1),\n","               'n_estimators': hp.qnormal('rfr_n_estimators', 100, 30, 1)}},\n","          {'model': GradientBoostingClassifier,\n","           'parameters': \n","           {'criterion': hp.choice('gbr_criterion', ['friedman_mse', \n","                                                     'squared_error', \n","                                                     'mse', 'mae']),\n","           'n_estimators': hp.qnormal('gbr_n_estimators', 100, 30, 1),\n","           'max_depth': hp.quniform('gbr_max_depth', 1, 30, q=1)}},\n","          {'model': AdaBoostClassifier,\n","           'parameters': {\n","               'n_estimators': hp.qnormal('abr_n_estimators', 50, 15, 1),\n","               'learning_rate': hp.uniform('abr_learning_rate', 1e-3, 10)}\n","          },\n","          {'model': MLPClassifier,\n","           'parameters':{'activation': hp.choice('mlp_activation', ['logistic', 'tanh', 'relu'])}\n","          }]\n","\n","space['models'] = hp.choice('models', models)\n","\n","def objective(params):\n","    cleanedParams = {}\n","    for param in params['models']['parameters']:\n","        value = params['models']['parameters'][param]\n","        if param == 'n_estimators':\n","            if value < 1:\n","                value = 1\n","            value = int(value)\n","        cleanedParams[param] = value\n","    \n","    model = params['models']['model'](**cleanedParams)\n","    model.fit(X_train, y_train)\n","    return -f1_score(model.predict(X_valid), y_valid)\n","\n","best = fmin(objective, space, algo=tpe.suggest, max_evals=100)"],"metadata":{"id":"Oc_UHdz4K2-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizing the training parameters of a neural network."],"metadata":{"id":"Myfld_dBeEE-"}},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n","from sklearn.metrics import f1_score\n","from hyperopt import hp\n","from hyperopt import fmin, tpe\n","\n","space = {}\n","\n","optimizers = [{'optimizer':SGD,\n","              'parameters':{\n","                  'learning_rate': hp.uniform('sgd_lr', 1e-5, 1),\n","                  'momentum': hp.uniform('sgd_mom', 0, 1),\n","                  'nesterov': hp.choice('sgd_nest', [False, True])\n","              }},\n","              {'optimizer':RMSprop,\n","              'parameters':{\n","                  'learning_rate': hp.uniform('rms_lr', 1e-5, 1),\n","                  'momentum': hp.uniform('rms_mom', 0, 1),\n","                  'rho': hp.normal('rms_rho', 1.0, 0.3),\n","                  'centered': hp.choice('rms_cent', [False, True])\n","              }},\n","              {'optimizer':Adam,\n","              'parameters':{\n","                  'learning_rate': hp.uniform('adam_lr', 1e-5, 1),\n","                  'beta_1': hp.uniform('adam_beta1', 0.3, 0.9999999999),\n","                  'beta_2': hp.uniform('adam_beta2', 0.3, 0.9999999999),\n","                  'amsgrad': hp.choice('amsgrad', [False, True])\n","              }},\n","              {'optimizer':Adagrad,\n","              'parameters':{\n","                  'learning_rate': hp.uniform('adagrad_lr', 1e-5, 1),\n","                  'initial_accumulator_value': hp.uniform('adagrad_iav', 0.0, 1.0)\n","              }}]\n","space['optimizers'] = hp.choice('optimizers', optimizers)\n","\n","\n","from keras.callbacks import ReduceLROnPlateau\n","space['lr_manage'] = {'factor': hp.uniform('lr_factor', 0.01, 0.95),\n","                      'patience': hp.quniform('lr_patience', 3, 20, q=1)}\n","\n","from keras.callbacks import EarlyStopping\n","\n","bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","def build_NN(input_dim = len(X_train.columns)):\n","    model = keras.models.Sequential()\n","    model.add(L.Input((input_dim,)))\n","    model.add(L.Dense(input_dim, activation='relu'))\n","    model.add(L.Dense(input_dim, activation='relu'))\n","    model.add(L.Dense(input_dim, activation='relu'))\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dense(16, activation='relu'))\n","    model.add(L.Dense(16, activation='relu'))\n","    model.add(L.Dense(16, activation='relu'))\n","    model.add(L.Dense(1, activation='sigmoid'))\n","    return model\n","    \n","    # later - build control over how many branches, etc.\n","\n","def objective(params):\n","    model = build_NN()\n","    es = EarlyStopping(patience=5)\n","    rlrop = ReduceLROnPlateau(**params['lr_manage'])\n","    \n","    optimizer = params['optimizers']['optimizer']\n","    optimizer_params = params['optimizers']['parameters']\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer=optimizer(**optimizer_params))\n","    model.fit(X_train, y_train, callbacks=[es, rlrop],\n","              epochs = 50, verbose = 0)\n","    valid_loss = bce(model.predict(np.array(X_valid).astype(np.float16)), \n","                     np.array(y_valid).reshape((len(y_valid),1)).astype(np.float16)).numpy()\n","    return valid_loss\n","\n","best = fmin(objective, space, algo=tpe.suggest, max_evals=100);"],"metadata":{"id":"LVETCkoSK2-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizing the architecture of a neural network."],"metadata":{"id":"c39X54zneI_O"}},{"cell_type":"code","source":["def build_NN(num_branches,\n","             num28repeats, num16repeats,\n","             join_method, numOutRepeats):\n","    inp = L.Input((28,))\n","    out_tensors = []\n","    for i in range(int(num_branches)):\n","        x = L.Dense(28, activation='relu')(inp)\n","        for i in range(int(num28repeats-1)):\n","            x = L.Dense(28, activation='relu')(x)\n","        for i in range(int(num16repeats-1)):\n","            x = L.Dense(16, activation='relu')(x)\n","        out_tensors.append(x)\n","    if num_branches == 1:\n","        join = out_tensors[0]\n","    elif join_method == 'concat':\n","        join = L.Concatenate()(out_tensors)\n","    else:\n","        join = L.Add()(out_tensors)\n","    x = L.Dense(16, activation='relu')(join)\n","    for i in range(int(numOutRepeats-1)):\n","        x = L.Dense(16, activation='relu')(x)\n","    out = L.Dense(1, activation='sigmoid')(x)\n","    return keras.models.Model(inputs=inp, outputs=out)\n","\n","space = {}\n","space['optimizers'] = hp.choice('optimizers', optimizers)\n","space['lr_manage'] = {'factor': hp.uniform('lr_factor', 0.01, 0.95),\n","                      'patience': hp.quniform('lr_patience', 3, 20, q=1)}\n","space['architecture'] = {'num_branches': hp.quniform('num_branches', 1, 5, q=1),\n","                         'num28repeats': hp.quniform('num28repeats', 1, 5, q=1),\n","                         'num16repeats': hp.quniform('num16repeats', 1, 5, q=1),\n","                         'join_method': hp.choice('join_method', ['add', 'concat']),\n","                         'numOutRepeats': hp.quniform('numOutRepeats', 1, 5, q=1)}\n","\n","def objective(params):\n","    model = build_NN(**params['architecture'])\n","    es = EarlyStopping(patience=5)\n","    rlrop = ReduceLROnPlateau(**params['lr_manage'])\n","    \n","    optimizer = params['optimizers']['optimizer']\n","    optimizer_params = params['optimizers']['parameters']\n","    model.compile(loss='binary_crossentropy',\n","                  metrics=['accuracy'],\n","                  optimizer=optimizer(**optimizer_params))\n","    model.fit(X_train, y_train, callbacks=[es, rlrop],\n","              epochs = 50, verbose = 0)\n","    valid_loss = bce(model.predict(np.array(X_valid).astype(np.float16)), \n","                     np.array(y_valid).reshape((len(y_valid),1)).astype(np.float16)).numpy()\n","    return valid_loss\n","\n","best = fmin(objective, space, algo=tpe.suggest, max_evals=100);\n","\n","best_model = build_NN(best['num_branches'],\n","                      best['num28repeats'],\n","                      best['num16repeats'],\n","                      space['architecture']['join_method'][best['join_method']],\n","                      best['numOutRepeats'])\n","keras.utils.plot_model(best_model, dpi=400, show_shapes=True)"],"metadata":{"id":"jPj47US4K2-U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizing regression for a classical machine learning model and its parameters (as opposed to classification, previously shown)"],"metadata":{"id":"OmajM7leeiad"}},{"cell_type":"code","source":["models = [{'model': LinearRegression,\n","           'parameters':{}},\n","          {'model': Lasso,\n","           'parameters': {'alpha': hp.uniform('lr_alpha', 0, 5),\n","                          'normalize': hp.choice('lr_normalize', [True, False])}},\n","          {'model': DecisionTreeRegressor,\n","           'parameters': {'criterion': hp.choice('dtr_criterion', ['squared_error', 'friedman_mse',\n","                                                'absolute_error', 'poisson']),\n","                          'max_depth': hp.quniform('dtr_max_depth', 1, 30, 1)}},\n","          {'model': RandomForestRegressor,\n","           'parameters': {\n","               'criterion': hp.choice('rfr_criterion', ['squared_error', 'friedman_mse',\n","                                                    'absolute_error', 'poisson']),\n","               'max_depth': hp.quniform('rfr_max_depth', 1, 30, q=1),\n","               'n_estimators': hp.qnormal('rfr_n_estimators', 100, 30, 1)}},\n","          {'model': GradientBoostingRegressor,\n","           'parameters': {'criterion': hp.choice('gbr_criterion', ['squared_error', 'absolute_error',\n","                                                'huber', 'quantile']),\n","           'n_estimators': hp.qnormal('gbr_n_estimators', 100, 30, 1),\n","           'criterion': hp.choice('gbr_criterion', ['squared_error', 'friedman_mse',\n","                                                'absolute_error', 'poisson']),\n","           'max_depth': hp.quniform('gbr_max_depth', 1, 30, q=1)}},\n","          {'model': AdaBoostRegressor,\n","           'parameters': {'n_estimators': hp.qnormal('abr_n_estimators', 50, 15, 1),\n","           'loss': hp.choice('abr_loss', ['linear', 'square', 'exponential'])}},\n","          {'model': MLPRegressor,\n","           'parameters':{'activation': hp.choice('mlp_activation', ['logistic', 'tanh', 'relu'])}}]\n","\n"," # build objective function\n","def objective(params):\n","    cleanedParams = {}\n","    for param in params['models']['parameters']:\n","        value = params['models']['parameters'][param]\n","        if param == 'n_estimators':\n","            value = int(value)\n","        cleanedParams[param] = value\n","    \n","    model = params['models']['model'](**cleanedParams)\n","    model.fit(X_train, y_train)\n","    return mae(model.predict(X_valid), y_valid)\n","\n","best = fmin(objective, space, algo=tpe.suggest, max_evals=1000)\n","best"],"metadata":{"id":"k2CJBYa1K2-U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimizing the Data Pipeline"],"metadata":{"id":"SRn95DYZK2-V"}},{"cell_type":"markdown","source":["Loading the Ames Housing dataset."],"metadata":{"id":"HhQGty2xepdl"}},{"cell_type":"code","source":["df = pd.read_csv('https://raw.githubusercontent.com/hjhuney/Data/master/AmesHousing/train.csv')\n","df = df.dropna(axis=1, how='any').drop('Id', axis=1)\n","x = df.drop('SalePrice', axis=1)\n","y = df['SalePrice']"],"metadata":{"id":"_PA_mD8xK2-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20, 10), dpi=400)\n","p = sns.displot(y, color='blue', height=8, aspect=15/8)\n","plt.show()"],"metadata":{"id":"kTJIAYHxK2-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizing the categorical encodings for each categorical feature."],"metadata":{"id":"xe2pFJ5zK2-V"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error as mae\n","\n","from category_encoders.ordinal import OrdinalEncoder\n","from category_encoders.one_hot import OneHotEncoder\n","from category_encoders.binary import BinaryEncoder\n","from category_encoders.target_encoder import TargetEncoder\n","from category_encoders.count import CountEncoder\n","from category_encoders.leave_one_out import LeaveOneOutEncoder\n","from category_encoders.james_stein import JamesSteinEncoder\n","from category_encoders.cat_boost import CatBoostEncoder\n","\n","space = {}\n","encoders = [[OrdinalEncoder(), False],\n","            [OneHotEncoder(), False],\n","            [BinaryEncoder(), True],\n","            [TargetEncoder(), True],\n","            [CountEncoder(), True],\n","            [LeaveOneOutEncoder(), True],\n","            [JamesSteinEncoder(), True],\n","            [CatBoostEncoder(), True]]\n","\n","cat_features = []\n","for colIndex, colName in enumerate(x.columns):\n","    # find categorical variables to process\n","    if type(x.iloc[0, colIndex]) == str or len(x[colName].unique()) <= 5:\n","        cat_features.append(colName)\n","        space[f'{colName}_cat_enc'] = hp.choice(f'{colName}_cat_enc', encoders)\n","        \n","# build objective function\n","def objective(params):\n","    x_ = pd.DataFrame()\n","    for colName in cat_features:\n","        colValues = np.array(x[colName])\n","        encoder = params[f'{colName}_cat_enc'][0]\n","        if params[f'{colName}_cat_enc'][1]:\n","            transformed = encoder.fit_transform(colValues, y)\n","        else:\n","            transformed = encoder.fit_transform(colValues)\n","        x_ = pd.concat([x_, transformed], axis=1)\n","    nonCatCols = [col for col in x.columns if (col not in cat_features)]\n","    x_ = pd.concat([x_, x[nonCatCols]], axis=1)\n","    \n","    X_train, X_valid, y_train, y_valid = tts(x_, y, train_size = 0.8, random_state = 42)\n","    \n","    model = RandomForestRegressor(random_state = 42)\n","    model.fit(X_train, y_train)\n","    return mae(model.predict(X_valid), y_valid)\n","\n","best = fmin(objective, space, algo=tpe.suggest, max_evals=1000);"],"metadata":{"id":"l1Kf3-77K2-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizing the categorical encoding for a neural network."],"metadata":{"id":"qwgjSi3pK2-W"}},{"cell_type":"code","source":["from sklearn.metrics import mean_absolute_error as mae\n","\n","from category_encoders.ordinal import OrdinalEncoder\n","from category_encoders.one_hot import OneHotEncoder\n","from category_encoders.binary import BinaryEncoder\n","from category_encoders.target_encoder import TargetEncoder\n","from category_encoders.count import CountEncoder\n","from category_encoders.leave_one_out import LeaveOneOutEncoder\n","from category_encoders.james_stein import JamesSteinEncoder\n","from category_encoders.cat_boost import CatBoostEncoder\n","\n","space = {}\n","encoders = [[OrdinalEncoder(), False],\n","            [OneHotEncoder(), False],\n","            [BinaryEncoder(), True],\n","            [TargetEncoder(), True],\n","            [CountEncoder(), True],\n","            [LeaveOneOutEncoder(), True],\n","            [JamesSteinEncoder(), True],\n","            [CatBoostEncoder(), True]]\n","\n","cat_features = []\n","for colIndex, colName in enumerate(x.columns):\n","    # find categorical variables to process\n","    if type(x.iloc[0, colIndex]) == str or len(x[colName].unique()) <= 5:\n","        cat_features.append(colName)\n","        space[f'{colName}_cat_enc'] = hp.choice(f'{colName}_cat_enc', encoders)\n","        \n","# build objective function\n","def objective(params):\n","    x_ = pd.DataFrame()\n","    for colName in cat_features:\n","        colValues = np.array(x[colName])\n","        encoder = params[f'{colName}_cat_enc'][0]\n","        if params[f'{colName}_cat_enc'][1]:\n","            transformed = encoder.fit_transform(colValues, y)\n","        else:\n","            transformed = encoder.fit_transform(colValues)\n","        x_ = pd.concat([x_, transformed], axis=1)\n","    nonCatCols = [col for col in x.columns if (col not in cat_features)]\n","    x_ = pd.concat([x_, x[nonCatCols]], axis=1)\n","    \n","    X_train, X_valid, y_train, y_valid = tts(x_, y, train_size = 0.8, random_state = 42)\n","    \n","    model = buildRegressionNN(input_dim = len(X_train.columns))\n","    model.fit(X_train.astype(np.float32), y_train, epochs = 20, verbose = 0)\n","    return mae(model.predict(X_valid), y_valid)\n","\n","def buildRegressionNN(input_dim):\n","    model = Sequential()\n","    model.add(L.Input((input_dim,)))\n","    for i in range(3):\n","        model.add(L.Dense(32, activation='relu'))\n","    for i in range(3):\n","        model.add(L.Dense(16, activation='relu'))\n","    model.add(L.Dense(1, activation='relu'))\n","    model.compile(optimizer='adam', loss='mse')\n","    return model\n","    \n","best = fmin(objective, space, algo=tpe.suggest, max_evals=1000);"],"metadata":{"id":"ddWSOmq6K2-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizing the model and the data pipeline."],"metadata":{"id":"FZ7lGP2-K2-W"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression, Lasso\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.metrics import mean_absolute_error as mae\n","\n","from category_encoders.ordinal import OrdinalEncoder\n","from category_encoders.one_hot import OneHotEncoder\n","from category_encoders.binary import BinaryEncoder\n","from category_encoders.target_encoder import TargetEncoder\n","from category_encoders.count import CountEncoder\n","from category_encoders.leave_one_out import LeaveOneOutEncoder\n","from category_encoders.james_stein import JamesSteinEncoder\n","from category_encoders.cat_boost import CatBoostEncoder\n","\n","space = {}\n","encoders = [[OrdinalEncoder(), False],\n","            [OneHotEncoder(), False],\n","            [BinaryEncoder(), True],\n","            [TargetEncoder(), True],\n","            [CountEncoder(), True],\n","            [LeaveOneOutEncoder(), True],\n","            [JamesSteinEncoder(), True],\n","            [CatBoostEncoder(), True]]\n","\n","cat_features = []\n","for colIndex, colName in enumerate(x.columns):\n","    # find categorical variables to process\n","    if type(x.iloc[0, colIndex]) == str or len(x[colName].unique()) <= 5:\n","        cat_features.append(colName)\n","        space[f'{colName}_cat_enc'] = hp.choice(f'{colName}_cat_enc', encoders)\n","space['model'] = hp.choice('model',\n","                           [LinearRegression, Lasso,\n","                            DecisionTreeRegressor, RandomForestRegressor,\n","                            GradientBoostingRegressor, AdaBoostRegressor,\n","                            MLPRegressor])\n","        \n","# build objective function\n","def objective(params):\n","    x_ = pd.DataFrame()\n","    for colName in cat_features:\n","        colValues = np.array(x[colName])\n","        encoder = params[f'{colName}_cat_enc'][0]\n","        if params[f'{colName}_cat_enc'][1]:\n","            transformed = encoder.fit_transform(colValues, y)\n","        else:\n","            transformed = encoder.fit_transform(colValues)\n","        x_ = pd.concat([x_, transformed], axis=1)\n","    nonCatCols = [col for col in x.columns if (col not in cat_features)]\n","    x_ = pd.concat([x_, x[nonCatCols]], axis=1)\n","    \n","    X_train, X_valid, y_train, y_valid = tts(x_, y, train_size = 0.8, random_state = 42)\n","    \n","    model = params['model']()\n","    model.fit(X_train, y_train)\n","    return mae(model.predict(X_valid), y_valid)\n","\n","# best = fmin(objective, space, algo=tpe.suggest, max_evals=1000);"],"metadata":{"id":"U7RTZxQ0K2-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizing everything - the encoding mechanisms, the machine learning model, the machine learning model parameters"],"metadata":{"id":"CCCQOO0UK2-X"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression, Lasso\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.metrics import mean_absolute_error as mae\n","\n","from category_encoders.ordinal import OrdinalEncoder\n","from category_encoders.one_hot import OneHotEncoder\n","from category_encoders.binary import BinaryEncoder\n","from category_encoders.target_encoder import TargetEncoder\n","from category_encoders.count import CountEncoder\n","from category_encoders.leave_one_out import LeaveOneOutEncoder\n","from category_encoders.james_stein import JamesSteinEncoder\n","from category_encoders.cat_boost import CatBoostEncoder\n","\n","space = {}\n","encoders = [[OrdinalEncoder(), False],\n","            [OneHotEncoder(), False],\n","            [BinaryEncoder(), True],\n","            [TargetEncoder(), True],\n","            [CountEncoder(), True],\n","            [LeaveOneOutEncoder(), True],\n","            [JamesSteinEncoder(), True],\n","            [CatBoostEncoder(), True]]\n","\n","models = [{'model': LinearRegression,\n","           'parameters':{}},\n","          {'model': Lasso,\n","           'parameters': {'alpha': hp.uniform('lr_alpha', 0, 5),\n","                          'normalize': hp.choice('lr_normalize', [True, False])}},\n","          {'model': DecisionTreeRegressor,\n","           'parameters': {'criterion': hp.choice('dtr_criterion', ['squared_error', 'friedman_mse',\n","                                                'absolute_error', 'poisson']),\n","                          'max_depth': hp.quniform('dtr_max_depth', 1, 30, 1)}},\n","          {'model': RandomForestRegressor,\n","           'parameters': {\n","               'criterion': hp.choice('rfr_criterion', ['squared_error', 'friedman_mse',\n","                                                    'absolute_error', 'poisson']),\n","               'max_depth': hp.quniform('rfr_max_depth', 1, 30, q=1),\n","               'n_estimators': hp.qnormal('rfr_n_estimators', 100, 30, 1)}},\n","          {'model': GradientBoostingRegressor,\n","           'parameters': {'criterion': hp.choice('gbr_criterion', ['squared_error', 'absolute_error',\n","                                                'huber', 'quantile']),\n","           'n_estimators': hp.qnormal('gbr_n_estimators', 100, 30, 1),\n","           'criterion': hp.choice('gbr_criterion', ['squared_error', 'friedman_mse',\n","                                                'absolute_error', 'poisson']),\n","           'max_depth': hp.quniform('gbr_max_depth', 1, 30, q=1)}},\n","          {'model': AdaBoostRegressor,\n","           'parameters': {'n_estimators': hp.qnormal('abr_n_estimators', 50, 15, 1),\n","           'loss': hp.choice('abr_loss', ['linear', 'square', 'exponential'])}},\n","          {'model': MLPRegressor,\n","           'parameters':{'activation': hp.choice('mlp_activation', ['logistic', 'tanh', 'relu'])}}]\n","\n","cat_features = []\n","for colIndex, colName in enumerate(x.columns):\n","    # find categorical variables to process\n","    if type(x.iloc[0, colIndex]) == str or len(x[colName].unique()) <= 5:\n","        cat_features.append(colName)\n","        space[f'{colName}_cat_enc'] = hp.choice(f'{colName}_cat_enc', encoders)\n","space['models'] = hp.choice('models', models)\n","\n","# build objective function\n","def objective(params):\n","    x_ = pd.DataFrame()\n","    for colName in cat_features:\n","        colValues = np.array(x[colName])\n","        encoder = params[f'{colName}_cat_enc'][0]\n","        if params[f'{colName}_cat_enc'][1]:\n","            transformed = encoder.fit_transform(colValues, y)\n","        else:\n","            transformed = encoder.fit_transform(colValues)\n","        x_ = pd.concat([x_, transformed], axis=1)\n","    nonCatCols = [col for col in x.columns if (col not in cat_features)]\n","    x_ = pd.concat([x_, x[nonCatCols]], axis=1)\n","    \n","    X_train, X_valid, y_train, y_valid = tts(x_, y, train_size = 0.8, random_state = 42)\n","    \n","    cleanedParams = {}\n","    for param in params['models']['parameters']:\n","        value = params['models']['parameters'][param]\n","        if param == 'n_estimators':\n","            if value < 1:\n","                value = 1\n","            value = int(value)\n","        cleanedParams[param] = value\n","    \n","    model = params['models']['model'](**cleanedParams)\n","    model.fit(X_train, y_train)\n","    return mae(model.predict(X_valid), y_valid)\n","\n","best = fmin(objective, space, algo=tpe.suggest, max_evals=500)"],"metadata":{"id":"fLwpRy3mK2-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## AutoKeras"],"metadata":{"id":"r9cGYQ51K2-X"}},{"cell_type":"code","source":["import autokeras as ak\n","input_node = ak.StructuredDataInput()\n","output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)\n","output_node = ak.ClassificationHead()(output_node)\n","clf = ak.AutoModel(\n","    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100\n",")\n","clf.fit(X_train, y_train, epochs=100) #, verbose=1)\n","keras.utils.plot_model(clf.export_model(), show_shapes=True, dpi=400)\n","model = clf.export_model()\n","bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","bce(model.predict(np.array(X_valid).astype(np.float16)), \n","                  np.array(y_valid).reshape((len(y_valid),1)).astype(np.float16)).numpy()\n","df = pd.read_csv('https://raw.githubusercontent.com/hjhuney/Data/master/AmesHousing/train.csv')\n","df = df.dropna(axis=1, how='any').drop('Id', axis=1)\n","x = df.drop('SalePrice', axis=1)\n","y = df['SalePrice']\n","\n","# X_train, X_valid, y_train, y_valid = tts(x, y, train_size = 0.8, random_state = 42)\n","\n","input_node = ak.StructuredDataInput()\n","output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)\n","output_node = ak.RegressionHead()(output_node)\n","clf = ak.AutoModel(\n","    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100\n",")\n","clf.fit(x, y, epochs=100)\n","keras.utils.plot_model(clf.export_model(), show_shapes=True, dpi=400)"],"metadata":{"id":"SbM_SZKGK2-X"},"execution_count":null,"outputs":[]}]}